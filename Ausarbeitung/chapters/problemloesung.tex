% ==============
% Anwendung auf das Phasenfeldmodell für Rissentstehung 
% ==============

\chapter{Anwendung auf das Phasenfeldmodell für Rissentstehung }

Nachdem wir die mathematischen Grundlagen für die Betrachtung eines Optimierungsproblems kennengelernt haben, wollen wir diese anwenden. Das Problem war gegeben durch 
\begin{align*}
	& \min\limits_{u \in  \h^2 , v \in \h } \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left(\epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) dx \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 \\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2  
\end{align*}
Zunächst teilen das Optimierungsproblem in zwei voneinander unabhängige Optimierungen auf: Die Optimierung nach $u$ und die Optimierung nach $v$. Im Anschluss betrachten wir beide Optimierungen genauer, indem wir sie umschreiben und numerische Verfahren zur Lösung entwickeln. Zum Schluss fusionieren wir beide Verfahren.


\section{Erste Betrachtung des Modells}


Beim genaueren Betrachten bemerkt man, dass die Ungleichungsnebenbedingung nur von $v$ und die Randbedingung nur von $u$ abhängt. Dies bietet die Möglichkeit das Optimierungsproblem in zwei Teilprobleme aufzuteilen.
\begin{align*}
\label{eq:problem_von_v}
	& \min\limits_{u \in\h^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2 
	\vspace{1ex} \\
	& \min\limits_{v \in\h} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 
\end{align*}

Wenn man beide Probleme implementiert, löst man zunächst die Optimierung nach $u$ und setzt die Lösung dann in die Optimierung nach $v$ ein. Danach setzt man die Lösung von $v$ in die Optimierung nach $u$ ein. Dieses Vorgehen wird mit der semidifferenzierbaren Newton Methode wiederholt. Betrachten wir zuerst die Optimierung nach u. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Optimierung nach u          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Optimierung nach u}

Das Kapitel ist in zwei Unterkapitel aufgeteilt: Die analytische und numerische Betrachtung. 

Im analytischen Teil formulieren wir das Minimierungsproblem zu einem Problem ohne Nebenbedingung um. Dieses lässt sich dann als partielle Differentialgleichung schreiben. Die Existenz und Eindeutigkeit der schwachen Lösung wird am Schluss des ersten Teils bewiesen. 

Für die numerische Betrachtung schreiben wir das Problem nochmal um und wenden dann Finite Elemente und den Galerkin Ansatz an. Das Resultat ist ein Gleichungssystem, das sich einfach lösen lässt. 

\subsection{Analytische Betrachtung}

Erinnern wir uns an das Optimierungsproblem, das die Verschiebung des Körpers bei der Entstehung von einem Riss beschreibt.
\begin{align*}
	& \min\limits_{u \in\h^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2
\end{align*}

Es ist leichter ein Problem ohne Nebenbedingung zu betrachten, also nehmen wir die Nebenbedingung mit in dem Raum auf, über den wir optimieren. Also suchen wir statt $u \in \h^2$ 
\begin{align*}
	u \in u_0 + \ho^2:=u_0+\{u \in\h^2| u=0 \text{ auf } \Gamma_1 \cup \Gamma_2\} 
\end{align*}
Das Problem hat dann folgende Form
\begin{align}
	\label{eq:optimierung_u_ohne_nb2}
	& \min\limits_{u \in u_0 +\ho^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x 
\end{align}

Da $u: \Omega \rightarrow \R^2$, müssen wir nach $u_1$ und nach $u_2$ minimieren. Es tauchen keine Mischungen aus den Termen $u_1$ und $u_2$ auf, das heißt, dass wir die Optimierungen trennen können. Beide sind identisch, es müssen später nur unterschiedliche Werte eingesetzt werden. Betrachten wir o.B.d.A. die Optimierung nach $u_1$. 

\begin{thm}[Bedingung für ein Minimum]
	Sei das Minimierungsproblem \eqref{eq:optimierung_u_ohne_nb2} gegeben und $\tilde{u_1}$  nimmt das Minimum an. Dann gilt 
	\begin{align}
		\label{eq:u_gleich_null}
		\int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right)  \nabla \tilde{u_1} \nabla  \psi \diff x = 0 \hspace{2ex}\forall  \psi \in u_0 + \ho 
	\end{align}
\end{thm}
\begin{proof}
	Nach \ref{thm:ableitung_gleich_null} muss nur überprüft werden, ob die Gâteaux-Ableitung von 
	\begin{align*}
		\begin{array}{rrcl}
			J: 	& u_0+ \ho^2& \rightarrow	& \R \\
			& u 											& \mapsto		& \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x 
		\end{array}
	\end{align*}
	von der Form \eqref{eq:u_gleich_null} ist. Leiten wir $J$ ab: 
	\begin{align*}
		\begin{array}{lcrl}
			\partial J(u)( \psi ) & = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big( & J(u_1+t  \psi,u_2)-J(u_1,u_2) \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla (u_1+t  \psi )|^2 + |\nabla u_2|^2 + \epsilon_2 |\nabla v|^2  \\ 
			& & &\hspace{-2ex} + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x \\
			& & &\hspace{-2ex} - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla u_1|^2 \hspace{7ex} + |\nabla u_2|^2  + \epsilon_2 |\nabla v|^2  \\
			& & &\hspace{-2ex} + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \left( | \nabla (u_1+t  \psi )|^2 - | \nabla u_1|^2 \right) \diff x  \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( | \nabla u_1+t \nabla  \psi |^2 - | \nabla u_1|^2 ) \diff x  
			 \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( | \nabla u_1|^2 + 2 t  \nabla u_1 \nabla  \psi  + t^2 |\nabla  \psi |^2 - | \nabla u_1|^2 ) \diff x   \\					
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( 2 t  \nabla u_1 \nabla  \psi  + t^2 |\nabla  \psi |^2 ) \diff x   \\	
			& = & &\int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right) \nabla u_1 \nabla  \psi \diff x  
		\end{array}	
	\end{align*}
	Damit dies eine G\^ateaux Ableitung ist, muss die Abbildung $J'(u_1):  \psi  \mapsto \partial J(u_1, \psi ) \in \R $ linear und beschränkt sein. Linearität ist einfach nachzurechnen. Beschränktheit lässt sich durch Cauchy-Schwarz zeigen.   
\end{proof}

Somit lautet unser analytisches Problem:
Finde $u_1 \in u_0+\ho$, sodass für alle $ \psi  \in u_0 +\ho$ gilt 
\begin{align}
\label{eq:problem_u}
	0 = \int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right)  \nabla u_1 \nabla  \psi   \diff x 
\end{align}
Nun ist noch interessant, ob eine Lösung existiert und ob sie eindeutig ist. Dieses hängt von $u_0$ und $v_0$ ab. 

\begin{thm}[Existenz und Eindeutigkeit]
	Sei $u_0 \in \h^2, ,v_0 \in \h$. Die schwache Lösung $u \in u_0+\ho$ von \eqref{eq:problem_u} existiert und ist eindeutig. 
\end{thm}
\begin{proof}
Wir wenden \ref{thm:existenz_schwache_loesung} an. Dazu müssen wir die Bilinearform aufstellen und alle  Annahmen aus \ref{ann:ex_und_eind} zeigen. 
Die Bilinearform lautet 
\begin{align*}
	\begin{array}{rrcl}
		B(u_1, \psi ): 	& \left( u_0 +\h0^2 \right)^2  & \rightarrow 	& \R \\
		& (u_1, \psi )											& \mapsto		& \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) 2  \nabla u_1 \nabla  \psi   \diff x 
	\end{array}
\end{align*}
Mit den Bezeichnungen aus Kapitel \ref{sec:grundlagen_pdgl} ist $g = u_0$, $f,b,c=0$ und  
\begin{align*}
	A(x):= \begin{pmatrix}
	v^2(x) + \epsilon_1 & 0 \\
	0 & v^2(x) + \epsilon_1 
	\end{pmatrix}		
\end{align*}
Aus \ref{ann:ex_und_eind} sind 3 und 4 bereits erfüllt, da $b,c=0$ gilt. Beweisen wir 1. 

Sei $\xi \in \R^n$. Dann gilt:
\begin{align*}
	\xi^T A(x) \xi  & = \xi^T  \begin{pmatrix}
	v^2(x) + \epsilon_1 & 0 \\
	0 & v^2(x) + \epsilon_1 
	\end{pmatrix} \xi		 \\
	& = (v^2+ \epsilon_1) \xi \cdot \xi \\
	& \ge \epsilon_1 |\xi|^2
\end{align*}
Damit ist Annahme 1 erfüllt mit $\lambda = \epsilon_1$. 
Für Annahme 2 gilt
\begin{align*}
	|\xi^T A(x) \zeta| =  (v^2 + \epsilon_1) \xi \cdot \zeta   \le (v_0^2 + \epsilon_2) \xi \cdot \zeta 
     \le (\sup (v_0)^2 + \epsilon_2)  |\xi| |\zeta| 
\end{align*}
mit $\Lambda = \sup (v_0)^2 + \epsilon   $. 
\end{proof}

\subsection{Numerische Betrachtung}

Das vorherige Kapitel hat das Minimierungsproblem zu folgender Nullstellensuchen vereinfacht: 

Finde $u_1 \in u_0+ H_0^1(\Omega)$, sodass
\begin{align*}
	0 = \int\limits_{\Omega} \left( v^2 + \epsilon_1 \right) \nabla u_1 \nabla  \psi \diff x \hspace{2ex} \forall  \psi \in u_0 + H_0^1(\Omega)
\end{align*}

Da die Nullstelle im Raum $H_0^1(\Omega)$ einfacher zu finden ist, als im Raum $u_0 + H_0^1(\Omega)$, stellen wir das Problem um. 
Dazu definieren wir $\uotild \in u_0 + \ho$, sodass $\uotild$ auf dem Rand $\Gamma_1 \cup \Gamma_2$  $u_{0_1}$ entspricht und sonst $0$ ist. Definiere zusätzlich $\utild \in \ho$, sodass $u_1=\utild + \uotild$. 
Damit lässt sich das Problem umschreiben zu 

Finde $\utild \in \ho$, sodass  $\forall  \psi \in \ho$  
\begin{align*}
	- \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla \uotild \nabla  \psi \diff x  = \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla \utild \nabla  \psi \diff x 
\end{align*}

Zur numerischen Betrachtung bieten sich Finite Elemente, insbesondere die dreieckig linearen Lagrange Elemente an. Dafür triangulieren wir das Gebiet, wie in \ref{sec:finite_elemente} dargestellt. Nun nutzen wir den Galerkin Ansatz. Dafür gilt ab jetzt $k:=(m+1)(n+1)$
\begin{align*}
	\utild(x,y):= \sum\limits_{i=1}^{k} u^h_i T_i(x,y) 
\end{align*}

Dabei sind  $T_i(x,y)$ die globalen Formfunktionen und $u^h_i$ die gesuchten Konstanten. Setzt man die Definition von $\utild$ ein und ersetzt $ \psi \in \ho$ durch die Basis von $P^*$, also den globalen Formfunktionen $T_i$, so gilt $\forall i \in \{1, \cdots , k\}$  
\begin{align*}
	& - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla \uotild \nabla  \psi \diff x  = \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla \utild \nabla  \psi \diff x \\
	\Leftrightarrow 
	& - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \sum\limits_{i=1}^{k} {u^h_0}_i \nabla T_i \nabla T_i \diff x  =\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \sum\limits_{i=1}^{k} u^h_i \nabla T_i  \nabla T_j  \diff x \\
	\Leftrightarrow
	& - \sum\limits_{i=1}^{k} {u^h_0}_i \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla T_j \nabla T_i \diff x
	= \sum\limits_{i=1}^{k} u^h_i \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)   \nabla T_i \nabla T_j  \diff x \\
	\Leftrightarrow
	& L*u_0^h =  L * u^h
\end{align*}
wobei $u^h:= (u^h_1, \cdots u^h_{k})^T $, $u_0^h:= ({u^h_0}_1, \cdots {u^h_0}_k)^T $ und $L:= \left( \int_{\Omega}  \left( v^2 + \epsilon \right)   \nabla T_i \nabla T_j  \diff x \right)_{ij} $

Also müssen wir $L$ berechnen und dann das Gleichungssystem $ L*u_0^h =  L * u^h$ lösen. 

\subsubsection{Berechnung des $u$ Integrals} 
Als erste Vereinfacherung betrachten wir nicht mehr das Integral über $\Omega$, sondern über die einzelnen Dreiecke der Triangulierung. Desweiteren ist $T_i$ linear, also $\nabla T_i$ konstant. Es gilt  
\begin{align*}
  \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla T_i \nabla T_j  \diff x  
 & = \sum\limits_{\tilde{E} \in E_k} \int\limits_{\tilde{E}}  \left( v^2 + \epsilon_1 \right) \nabla T_i \nabla T_j  \diff x \\
 & =  \sum\limits_{\tilde{E} \in E_k} \nabla T_i \nabla T_j \int\limits_{\tilde{E}}  \left( v^2 + \epsilon_1 \right)   \diff x 
\end{align*}

Wir kennen $\nabla T_i \nabla T_j$ auf jedem Dreieck. Also muss nur noch $\int_E v^2+ \epsilon \diff x$ berechnet werden. Es darf über das Referenzdreieck integriert werden, da durch den Transformationssatz das Integral über das transformierte Element gewonnen werden kann. Es gilt:
\begin{align*}
	\int\limits_E v^2 + \epsilon_1 \diff x &  = \int\limits_E v^2  \diff x + \frac{1}{2}\epsilon_1  \\
\end{align*}
Da $v$ bereits numerisch berechnet wurde, haben wir nur Funktionsauswertungen von $v$ an den Ecken des Dreieckes gegeben und wir wissen, dass $v \in \mathcal{P}_1$. Also ist v eindeutig bestimmt und kann berechnet werden. Die Darstellung von $v$ ist in \eqref{eq:gerades_dreieck_lineare_fkt} und \eqref{eq:ungerades_dreieck_lineare_fkt} zu finden. 

Die Berechnung von $\int_E v^2 \diff x$ sieht wie folgt aus: 
\begin{align*}
	\int\limits_E v(x,y)^2 \diff x \diff y 
	& = \int\limits_0^1 \int\limits_0^{1-y}
	 \left( 
	 	(v_3-v_1)x+(v_2-v_1)y+v_1
	 \right)^2
	 \diff x \diff y \\ 
	 & = \frac{1}{12} (v_1^2+v_2^2+v_3^2 + v_1v_2 + v_1 v_3 + v_2 v_3)
\end{align*}
Da die Berechnung über das transformierte Element durchgeführt wurde, muss noch der Multiplikator $\frac{1}{h_1 h_2}$ eingefügt werden. 

Berechnen wir nun 
\begin{align*}
L_{i,j} = \sum\limits_{\tilde{E} \in E_k} \nabla T_i \nabla T_j \int\limits_{\tilde{E}}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}
wobei $T_i$ auf dem Gitterpunkt $i$ den Wert $1$ hat und sonst den Wert $0$ annimmt. Das heißt genauer, dass $T_i$ nur auf sechs Dreiecken ungleich 0 ist. Um das Integral zu bestimmen, braucht man also maximal sechs Dreiecke. Falls der Gitterpunkt am Rand liegen sollte, betrachtet man nur drei Dreiecke, da die Dreiecke hinter dem Rand 0 gesetzt werden. An den Ecken berechnet man aus den selben Grund entweder ein oder zwei Dreiecke. 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/triang_inner.png}
	\caption{Triangulierung im Inneren}
	\label{fig:triang_inner}
\end{figure}

Jetzt können wir $L_{ij}$ für festes $i,j$ berechnen. Falls $i$ und $j$ nicht adjazent sind, ist $L_{ij}=0$, da $T_i T_j=0$ gilt. Seien nun $T_I$ und $T_j$ adjazent. Hier haben wir vier Fälle:
\begin{itemize}
	\item  $i$ liegt auf $j$, also $j=i$ 
	\item  $j$ liegt rechts neben $i$ also $j=i+1$
	\item $j$ liegt direkt unter $i$, $j=i+n+1$ 
	\item  $j$ liegt schräg unter $i$, also $j=i+n+2$
\end{itemize}
Betrachten wir für die einzelnen Berechnungen Abbildung \ref{fig:triang_inner}. $T_i$ ist immer der Mittelpunkt dieser Zeichnung, $T_j$ ist entsprechend des jeweiligen $j$ positioniert. In den Berechnungen stimmen die Nummerierungen der Dreiecke mit den Nummerierungen in der Abbildung \ref{fig:triang_inner} überein und $\varphi_k$ mit $k \in \{1,2,3\}$ entspricht den $\varphi_k$ in \eqref{eq:varphi}. 

Betrachten wir nun die vier Fälle. 

\paragraph*{$i$ und $j$ sind gleich}
\begin{align*}
	L_{i,i}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_i   \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x\\
		& =  \nabla \varphi_1 \nabla \varphi_1 \int\limits_{E_1}  \left( v^2 + \epsilon \right)   \diff x
		+  \nabla \varphi_2 \nabla \varphi_2  \int\limits_{E_2}  \left( v^2 + \epsilon \right)  \diff x\\
		& +  \nabla \varphi_0 \nabla \varphi_0 \int\limits_{E_3}  \left( v^2 + \epsilon \right)   \diff x 
		+ \nabla \varphi_0 \nabla \varphi_0 \int\limits_{E_4} \left( v^2 + \epsilon \right) \diff x \\ 
		& +  \nabla \varphi_2  \nabla \varphi_2  \int\limits_{E_5}  \left( v^2 + \epsilon \right) \diff x	
		+ \nabla \varphi_1 \nabla \varphi_1 \int\limits_{E_6} \left( v^2 + \epsilon \right) \diff x 
\end{align*}

\paragraph*{$j$ liegt rechts neben $i$}
\begin{align*}
	L_{i,i+1}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+1}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_3}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_6}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}

\paragraph*{$j$ liegt unter $i$}
\begin{align*}
%korrekte varphi?
	L_{i,i+1+n}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+1+n}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_0 \nabla \varphi_1 \int\limits_{E_4}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_5}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}

\paragraph*{$j$ liegt schräg unter $i$}
\begin{align*}
%korrekte varphi?
	L_{i,i+2+n}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+2+n}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_1 \nabla \varphi_2 \int\limits_{E_5}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_1\nabla \varphi_2  \int\limits_{E_6}  \left( v^2 + \epsilon \right)   \diff x \\
 				& = 0
\end{align*}

\paragraph*{Zusammenfassung}
Mit diesen Werten können wir die Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ aufstellen:

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/matrix.png}
	\caption{Darstellung der Form der Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ }
	\label{fig:int_u}
\end{figure}
Dabei sind auf der Diagonalen die Einträge $L_{i,i}$, auf der Nebendiagonalen die Einträge $L_{i,i+1}$ und auf der anderen Diagonale die Einträge $L_{i,i+n+1}$. Wir haben bis jetzt immer nur über das Referenzdreieck integriert. Da wir aber eigentlich über die transformierten Dreiecke integrieren, müssen wir die Matrix mit dem Faktor $1/(h_1 h_2)$ multiplizieren. 

\subsection{Aggregation}

Nun haben wir die Matrix $L$ und den Vektor $u_0^h$ gegeben, um das Gleichungssystem $L u^h = L u_0^h$ zu berechnen. Allerdings haben wir noch nicht eingebracht, dass auf $\Gamma_1 \cup \Gamma_2$ $u=0$ gilt. Eigentlich würden wir das im Vektor $u$ mit aufnehmen, also die Zeilen 0 setzen, die den Rand repräsentieren und dann das Gleichungssystem lösen. Dies geht numerisch jedoch nicht so einfach. Die Information muss in $L$ und in $L u_0^h$ codiert sein. Dazu setzt man die Zeilen in $L$ gleich 0, die zum Rand gehören. Die zugehörigen Diagonaleinträge werden 1 gesetzt. Diese Matrix nennen wir $\tilde{L}$. Die zugehörige Einträge in $L u_0^h$, also die Einträge, die in der selben Zeile sind, setzt man 0. Den neuen Vektor nennen wir $\tilde{L u_0^h}$ Dadurch erhält man, dass $u^h$ an dieser Stelle 0 wird. 


Damit haben wir beide Seiten diskretisiert und können das Gleichungssystem implementieren.  
Wir wollen
\begin{align*}
	\frac{1}{h_1 h_2}\tilde{L} u=\frac{1}{h_1 h_2} \tilde{L u_0^h} \Leftrightarrow \tilde{L}u = \tilde{L u_0^h}
\end{align*}
berechnen. Der Code dazu hat folgende Form 

\begin{algorithm}[H]
	\caption{Berechnung von u}
	1. Berechne Matrix $\tilde{L}$ \\
	2. Berechne Vektor $\tilde{L u_0^h}$ \\
	3. $u = \tilde{L u_0^h}\backslash \tilde{L}$ 
\end{algorithm}
Da sowohl $\tilde{L}$ als auch $\tilde{L u_0^h}$ aus fast nur Nullen besteht, verwende ich in Matlab Sparse Matrizen, also dünn besetzte Matrizen, mit denen Matlab sehr effizient rechnen kann. Dies führt zu einer wesentlich besseren Laufzeit. 

\section{Optimierung nach v}

Bei der Optimierung nach $v$ geht es um die Fortsetzung des Risses. Für dieses Optimierungsproblem mit Ungleichungsnebenbedingung sicheren wir zunächst die Existenz und Eindeutigkeit der Lösung. Danach stellen wir die Optimalitätsbedingungen auf. Das resultiert in einem Karush Kuhn Tucker System. Dieses wollen wir mittels semidifferenzierbarer Newton Methode lösen. Dazu müssen zunächst alle Funktionen des KKT Systems differenziert und danach diskretisiert werden. Dies geschieht wieder mit Finiten Elementen. Am Schluss führen wir beide Optimierungsprobleme zu einem Verfahren zusammen.    

\newpage 
\subsection{Analytische Betrachtung}
Die Optimierung nach $v$ hat folgende Form:

\begin{align}
	\label{eq:problem_von_v}
	& \min\limits_{v \in\h} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 \notag
\end{align}
Das lässt sich allgemein als Optimierungsproblem mit Ungleichungsnebenbedingungen darstellen  
\begin{align*}
 \min\limits_{w \in W} J(w) \quad \text{s.d.} \quad w \in C 
\end{align*}
wobei W ein Banachraum, $J: W \rightarrow \R $ Gâteaux-differenzierbar und $C \subset W$. 
In diesem Fall bedeutet das:
\begin{align*}
	\begin{array}{lrcl}
		J: 	& H^1(\Omega)	& \rightarrow 	& \R \\
			& v				& \mapsto		& \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
		\multicolumn{4}{l}{ C:= \left\{ v \in H^1(\Omega) | 0 \le v \le v_0 \right\} }	
	\end{array}
\end{align*}
Zunächst wollen wir die Existenz und Eindeutigkeit der Lösung zeigen. Dafür brauchen wir die  Gâteaux differenzierbarkein von $J$. 
\begin{lem}
	\label{lem:J_g_diffbar}
	$J$ ist Gâteaux differenzierbar mit 
	\begin{align*}
		\begin{array}{lrcl}
			J'(v): 	& \overline{H^1}(\Omega) 	& \rightarrow 	& \R \\
			&\psi_1							& \mapsto		&  \int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x\\
		\end{array}	
	\end{align*}
\end{lem}
\begin{proof}
	Zunächst müssen wir die Richtungsableitung bestimmen.   
	\begin{align*}
		\begin{array}{lcrl}
			\partial J(v)(\psi_1) & = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big( & J(v+t\psi_1)-J(v) \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( (v+t\psi_1)^2 + \epsilon_1 \right) | \nabla (u)|^2 \\ 
			& & &\hspace{-2ex} + \nu \left( \epsilon_2 |\nabla (v+t\psi_1)|^2 + \frac{1}{\epsilon_2} \left( 1- (v+t\psi_1) \right)^2 \diff x \right) \\
			& & &\hspace{-2ex} - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla u|^2 \hspace{8ex} \\
			& & &\hspace{-2ex} + \nu \left( \epsilon_2 |\nabla v|^2 \hspace{8ex}+ \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \Big)  \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega} \left( \left( (v+t\psi_1)^2 + \epsilon_1 \right) - \left( v^2 + \epsilon_1 \right) \right) | \nabla (u)|^2 \\
			& & & \hspace{2ex} + \nu \left( \epsilon_2 \left( |\nabla (v+t\psi_1)|^2 - |\nabla v|^2 \right) \right. \\
			& & & \hspace{2ex} \left. + \frac{1}{\epsilon_2} \left( (1- v- t\psi_1)^2 -  (1- v)^2 \right) \right) \diff x	\Big) \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( v^2 + 2vt\psi_1 + t^2\psi_1^2 - v^2  \right) | \nabla u|^2 \\
			& & & \hspace{2ex} + \nu \left( \epsilon_2 \left( |\nabla v|^2 + 2 t \nabla v \nabla \psi_1 + t^2 |\nabla \psi_1|^2 - |\nabla v|^2 \right) \right. \\
			& & & \left. \hspace{2ex} + \frac{1}{\epsilon_2} \left( (1- v)^2 - 2(1-v)t\psi_1 + t^2\psi_1^2 -  (1- v)^2 \right) \right)
			\Big) \\		
			& = &\lim\limits_{t \rightarrow 0} 
			\Big(	& 
			\int\limits_{\Omega}  \left( 2v\psi_1 + t\psi_1^2  \right) | \nabla u|^2 + \nu \left( \epsilon_2 \left( 2 \nabla v \nabla \psi_1 + t |\nabla \psi_1|^2 \right) \right.\\
			& & & \hspace{2ex} \left.  - \frac{1}{\epsilon_2} \left(2(1-v)\psi_1 + t\psi_1^2 \right) \right) \diff x
			\Big) \\	
			& = &\hspace{6.5ex} & 
			\int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x\\	
			& = &\hspace{6.5ex} & 
			\int\limits_{\Omega} 2  v | \nabla u|^2 \psi_1 - \nu \left( \epsilon_2 2 \Delta v  \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x 	\\
			& & &\hspace{-2ex} + \int\limits_{\partial \Omega} 2 \nu  \epsilon_2 \nabla v \nu \psi_1 \diff x  						 
		\end{array}
	\end{align*}
	Damit es auch eine G\^ateaux Ableitung ist, muss sie beschränkt und linear sein. Dies ist einfach zu sehen. 
\end{proof}

\begin{thm}
Das Problem  \eqref{eq:problem_von_v} besitzt genau eine Lösung, falls $v_0$ stetig ist. 
\end{thm}
\begin{proof}
Wir wollen Theorem \ref{thm:existenz_eindeutigkeit_loesung} anwenden. Zunächst müssen wir alle Voraussetzungen prüfen. 
\begin{enumerate}
	\item  $W=H^1(\Omega)$ ist ein Hilbertraum, also auch ein reflexiver Banachraum. 
	\item Nun muss gezeigt werden, dass $C$ nichtleer, abgeschlossen und konvex ist. 
$C$ ist nichtleer, da $0 \in C$. 

Sei $v_n$ eine konvergente Folge in C. Dann gilt $0 \le v_n \le v_0 \hspace{1ex}$ für alle $ n \in \N$. Es gilt auch $0 \le \lim_{n \rightarrow \infty} u_n \le v_0$. Also ist $C$ abgeschlossen. 

Um Konvexität von C zu zeigen sei $0<\lambda<1 $ und $v, w \in C$. Dann gilt \mbox{ $0 \le \lambda v + (1- \lambda) w$}, da $\lambda>0$. Außerdem gilt $\lambda v + (1- \lambda) w \le \lambda v_0 + (1- \lambda) v_0 = v_0$. Also ist jede Konvexkombination in $C$ enthalten, $C$ ist konvex. 
	\item $J$ ist strikt konvex. Der Beweis dazu kann durch einfaches Nachrechnen geführt werden. Für Stetigkeit gilt dasselbe. 
	\item $J$ ist Gâteaux differenzierbar nach \eqref{lem:J_g_diffbar}
	\item Sei $w \in C$ mit $\|v\|_{H^1(\Omega)} \rightarrow \infty$. Dann gilt
\begin{align*}
	\begin{array}{ll}
		J(v) 	& = \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
		& = \int_{\Omega} v^2  | \nabla u|^2 + \epsilon_1  | \nabla u|^2  + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- 2v + v^2 \right) \right) \diff x \\
		& = \int_{\Omega} v^2  | \nabla u|^2 - \nu \left( \frac{2}{\epsilon_2} v + \frac{1}{\epsilon_2}v^2 \right)  \diff x + \int_{\Omega} \epsilon_1  | \nabla u|^2  + \nu \frac{1}{\epsilon_2}\diff x  \\
		& \hspace{2ex}+  \int_{\Omega} \nu \epsilon_2 |\nabla v|^2 \diff x \\
		& \le \int_{\Omega} v^2  \left(| \nabla u|^2 + \nu \frac{1}{\epsilon_2} \right)   \diff x + c  +  \epsilon_2 \| \nabla v\|_{L^2(\Omega)}^2 \\
		& \le c' \|v\|_{L^2(\Omega)}^2 + c + \epsilon_2 \| \nabla v\|_{L^2(\Omega)}^2 \\
		& \le c'' \left( \|v\|_{L^2(\Omega)}^2 + \| \nabla v\|_{L^2(\Omega)}^2 \right) + c \\
		& \le c'' \|v\|_{H^1(\Omega)}^2   + c \\
		& \rightarrow \infty
	\end{array}
\end{align*}
mit $c,c',c''>0$ passende Konstanten. 
\end{enumerate}
Alle Voraussetzungen aus Theorem \ref{thm:existenz_eindeutigkeit_loesung} sind erfüllt, also existiert genau eine Lösung des Optimierungsproblems. 
\end{proof}
  
Nachdem wir nun wissen, dass die Lösung existiert und eindeutig ist, wollen wir das Minimum finden. Dazu brauchen wir Optimalitätsbedingungen. Diese stellt das folgende Theorem auf:  

\begin{thm}[Optimalitätsbedingungen]
	Sei $a:=\inf \{J(w)|H(w) \le_P 0\}$. Dann gilt:
	\begin{align*}
		a= \inf\limits_{v \in H^1(\Omega)} J(v)+ \langle H(v), \begin{pmatrix} \lambda \\ \mu \end{pmatrix} \rangle_{H^1(\Omega), H^{-1}(\Omega)}
	\end{align*} 
	mit 
	\begin{align*}
		H:H^1(\Omega) \rightarrow H^1(\Omega) \\
		v \mapsto	\begin{pmatrix} -v \\ v-v_0 \end{pmatrix}
	\end{align*}
\end{thm}
\begin{proof}
	Die Bedingungen aus \ref{thm:kkt_system} müssen gelten.
	Sei 
	\begin{align*}
			P:=\{(v,w) \in H^1(\Omega) \times H^1(\Omega) | v\ge 0 \text{ und } w \ge 0\} \subset H^1(\Omega) \times H^1(\Omega)
	\end{align*}
	$\mathring{P} \neq \emptyset$, da $H^1(\Omega)$ nur stetige Funktionen enthält. Also ist $P$ ein positiver Kegel. 
	
	$J:H^1(\Omega) \rightarrow \R $ sei wie oben definiert. 
	$H$ ist linear, also konvex. 
	
	Das Bild von $J$ enthält ein $\hat{v}$, sodass $H(\hat{v})<_P 0 $ gilt, da es ein $v \in H^1(\Omega)$ geben muss, das echt zwischen $0$ und $v_0$ liegt. 
	
	Außerdem ist $a:=\inf \{J(w)|H(w) \le_P 0\}< \infty$, da J stetig und beschränkt ist. 
	
	Also kann Theorem \ref{thm:kkt_system} angewendet werden. Damit existiert $(\mu, \lambda) \in H^{-1}(\Omega) \times H^{-1}(\Omega)$ mit $(\mu,\lambda) \ge 0$ komponentenweise, sodass 
	\begin{align*}
		a= \inf\limits_{v \in H^1(\Omega)} J(v)+ \langle H(v), \begin{pmatrix} \lambda \\ \mu \end{pmatrix} \rangle_{H^1(\Omega), H^{-1}(\Omega)}
	\end{align*}
\end{proof}

Damit müssen wir nur noch das Minimum der Lagrangefunktion suchen. Dies funktioniert, indem wir die Ableitung bestimmen und 0 setzen. Wir leiten die Lagragefunktion ab und erhalten  $\nabla J(v)+ \lambda - \mu=0$. Ausformuliert lautet das Problem: 
\begin{align*}
	& 2  v | \nabla u|^2  - \epsilon_2 2 \Delta v   - \frac{2}{\epsilon_2} (1-v) + \lambda - \mu = 0 & \text{ auf } \Omega \\
	& 2 \epsilon_2 \nabla v \nu  =0 & \text{ auf } \partial \Omega
\end{align*} 

Nun sind alle Voraussetzungen erfüllt, damit das KKT System aufgestellt werden kann. 
\begin{align*}
	\begin{array}{lll}
	 	\multicolumn{3}{l}{  2  \overline{v} | \nabla u|^2  - \epsilon_2 2 \Delta \overline{v}   - \frac{2}{\epsilon_2} (1-\overline{v}) + \lambda - \mu = 0 \text{ auf } \Omega } \\
	 	\multicolumn{3}{l}{2 \epsilon_2 \nabla \overline{v} \nu  =0 \text{ auf } \partial \Omega} \\
	 	\overline{v} \ge a & \mu \ge 0 & \mu \overline{v}=0 \\
	 	\overline{v} \le b & \lambda \ge 0 & \lambda (v_0-\overline{v})=0 \\
	\end{array}
\end{align*}

Die Projektion für die Nebenbedingung lautet nach \eqref{eq:min_max_theorie}:
\begin{align*}
\mu - \lambda = \max\{0, \mu- \lambda + c(\overline{v}-v_0)\}+ \min\{0, \mu- \lambda + c\overline{v}\} \vspace{2ex} \forall c>0
\end{align*}

Daraus ergibt sich eine starke und schwache Formulierung. Die Starke lautet:  
 Suche $v \in H^1$, sodass 
\begin{align*}
	\begin{array}{lll}
		\multicolumn{3}{l}{  2  \overline{v} | \nabla u|^2  - \epsilon_2 2 \Delta \overline{v}   - \frac{2}{\epsilon_2} (1-\overline{v}) + \eta = 0 \text{ auf } \Omega } \\
		\multicolumn{3}{l}{2 \epsilon_2 \nabla \overline{v} \nu  =0 \text{ auf } \partial \Omega} \\
		\eta = \max\{0, \eta + c(\overline{v}-v_0)\}+ \min\{0, \eta + c\overline{v}\} \hspace{1ex} \forall c>0
	\end{array}
\end{align*}
Die schwache Formulierung ist dann
\begin{align*}
 	& \int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1 \diff x = 0  \\
	& \int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(\overline{v}-v_0)\}- \min\{0, \eta + c\overline{v}\} \right) \psi_1 \diff x = 0 
\end{align*}
$ \forall c>0, \forall  \psi_1 \in \h $ mit $\eta = \mu - \lambda$

\subsection{Anwendung auf semidifferenzierbare Newton Methode}

Unser Ziel ist es, eine Methode zu finden, mit der das KKT System lösen können. Betrachten wir also 
\begin{align*}
	%\begin{array}{rcl}
		& G: H^1(\Omega) \times H^1(\Omega) \rightarrow  H^{-1}(\Omega)^2 \\
		& (v, \eta)								\mapsto   
		\begin{pmatrix}
			\int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1 \right) \diff x  \\
			\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
		\end{pmatrix}
	%\end{array}
\end{align*}
Wir wollen $(v, \eta)$ finden, sodass $G=0$ ist. Direkt kann diese Formel nicht gelöst werden, da wir, um $G_2$ zu berechnen, $(v,\eta)$ benötigen. Dies ist nicht gegeben. Also lösen wir das Problem mit einer Newton Methode. Für jeden Iterationsschritt ist $(v,\eta)$ durch den vorherigen gegeben. Für die Newton Methode wird die Ableitung von $G$ benötigt. Da $G_2$ offensichtlich keine G\^ateaux-Ableitung hat, muss das Semidifferenzial berechnet werden. Daraus folgt, dass die semidifferenzierbare Newton Methode angewendet werden muss. 

Sehen wir uns zunächst die $\partial G_1(v,\eta)(h)$ an. Es gilt:

\begin{thm}
	\label{thm:G1_semidiffbar}
	$G_1(v,\eta)$ ist semidifferenzierbar mit 
	\begin{align*}
		& \partial G_{1 v} (v, \eta)(\phi_1) = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& \partial G_{1 \eta} (v, \eta) (\phi_2) = \int\limits_{\Omega} \nu  \phi_2 \psi_1  \diff x
	\end{align*}	
	falls $u$ fest gewählt ist, oder $u \in L^{\infty}$.
\end{thm}
\begin{proof}
	Nach Lemma \ref{lem:semidifferenzierbar_f_differenzierbar} ist $G_1$ $\partial G_1$ semidifferenzierbar, falls $G_1$ stetig Fréchet differenzierbar ist. Bestimmen wir zunächst die Richtungsableitung in Richtung $\phi_1$ bzw $\phi_2$. Diese ist gegeben durch 
	\begin{align*}
		& \partial G_{1 v} (v, \eta)(\psi_1,\phi_1) = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& G_{1 \eta} (v, \eta) (\phi_2) = \int\limits_{\Omega}  \nu \phi_2 \psi_1  \diff x
	\end{align*}
	Die Berechnung wird hier nicht weiter ausgeführt. Die Richtungsableitungen müssen linear und beschränkt sein. 
	Linearität ist einfach zu sehen.
	Für Beschränktheit von $\partial G_{1v}(v,\eta)( \psi_1,\phi_1 ) $ gilt: 
	\begin{align*}
		\|\partial G_{1v}(v,\eta)(\psi_1,\phi_1)\|_{H^{-1}} \le & = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 +\nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& \le 2 \|\nabla u\|^2 |(\psi_1,\phi_1)_{H^1}|+  \nu \max\{2 \epsilon_2, \frac{2}{\epsilon_2}\} |(\psi_1, \phi_1)_{H^1}| \\
		& \le \left( C + \tilde{C} \|\nabla u\|^2 \right)\|\psi_1\|_{H^1} \|\phi_1\|_{H^1}
	\end{align*}
	Da $u \in L^{\infty}$, ist $\|\nabla u\|^2$ beschränkt.  Damit ist $G_{1v}$ beschränkt. 
	Für Fréchet Differenzierbarkeit muss eine Abschätzung überprüft werden:
	\begin{align*}
		& \|G_1(v + h,\eta) - G_1(v,\eta) - \partial G_{1v}(v,\eta)(h)\| \\
		& =  \| 	\int\limits_{\Omega} 2 \psi_1  (v+h)  | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla (v+h) \nabla \psi_1  - \frac{2}{\epsilon_2} (1-(v+h))\psi_1 + \eta \psi_1  \right)\diff x \\
		& \hspace{3ex} - 	\int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1  \right) \diff x \\
		&  \hspace{3ex} - \int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1  \right) \diff x \| \\
		& = \| \int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1 \right) \diff x -\int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 \\
		& \hspace{3ex}+ \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1 \right) \diff x \| \\
		& = 0
	\end{align*} 
	Da $G_{1v}$ beschränkt und linear ist, ist $G_{1v}$ auch stetig. 
\end{proof}

Für das Semidifferenzial von $G_2$  beweisen wir zunächst ein Lemma. 

\begin{lem}
	Betrachte $f: \h^2 \rightarrow H^{-1}(\Omega)$ mit 
	\begin{align}
		\label{eq:H}
		(\eta, v ) \mapsto  \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\}
	\end{align}
	Dann ist $f$ semidifferenzierbar mit 
	\begin{align*}
		\frac{\partial f}{\partial \eta} = 
		\left\{
		\begin{array}{ll}
			\{0\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{1\} 		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack 0,1 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right .
	\end{align*}	
	und 
	\begin{align*}
		\frac{\partial f}{\partial v}= 
		\left\{
		\begin{array}{ll}
			\{-c\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{0\}		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack -c,0 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right .
	\end{align*}	
\end{lem}
\begin{proof}
	$f$ kann in einer anderen Form dargestellt werden 
	\begin{align*}
		f(v, \eta) = 
		\left\{
		\begin{array}{ll}
			-c(v-v_0) 	& \text{ falls }  -c(v-v_0) \le \eta \\
			\eta 		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			-cv		& \text{ falls }  \eta \le -c v 
		\end{array}
		\right .
	\end{align*}
	Die Äquivalenz dieser Form von $f$ und \eqref{eq:H}, kann einfach nachgerechnet werden. 
	Betrachten wir zunächst die Ableitung nach $\eta$. Es reicht, die Semidifferenzierbarkeit der einzelnen Abschnitte zu betrachten. Falls jeder Abschnitt semidifferenzierbar ist und die Übergänge auch, so ist $f$ semidifferenzierbar. 
	
	Sei dazu $-c(v-v_0) < \eta$ oder $ \eta < -c v $. Mit Lemma \ref{lem:semidifferenzierbar_f_differenzierbar} gilt die $\partial f$ Semidifferenzierbarkeit von f, falls $f$ stetig Fréchet Differenzierbar ist. Um Fréchet Differenzierbarkeit zu zeigen, bestimmen wir zunächst die Richtungsableitung. Diese ist offensichtlich $0$. Dadurch folgt sofort die Fréchet Differenzierbarkeit. 
	
	Sei nun $-cv < \eta < -c(v-v_0) $. Durch Lemma \ref{lem:semidifferenzierbar_f_differenzierbar} müssen wir wieder die Fréchet Differenzierbarkeit überprüfen. Offensichtlich ist die Identität Fréchet differenzierbar. Das Differenzial ist hier 1. 
	
	Sei $\eta =-c(v-v_0)$. Sei zunächst $d>0$. Die Abschätzung \eqref{eq:semidifferenzierbar_abschaetzung} muss gelten. Hier ist $\partial f(\eta+d, v) = \{0\}$ und damit 
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex}= \| -c(v-v_0) +c(v-v_0) \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right)  \text{ für } \| d \|_{\h} \rightarrow 0
	\end{align*}	
	Sei nun $d<0$. Da $d$ nahe an 0 ist, gilt $d>-cv_0$ mit $v_0>0$. Es ist $\partial G_2^{\eta}(\eta+d) = \{1\}$ und damit 
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta,v)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex} = \| -c(v-v_0) +d +c(v-v_0) -d \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right) 
	\end{align*}	
	für $\| d \|_{\h} \rightarrow 0$
	Es fehlt noch $\eta = -cv$. Sei zunächst $d>0$. Da $d$ nahe an 0 ist, gilt auch $d<cv_0$. Es gilt $\partial f(\eta+d,v) = \{1\}$ und damit 	
	\begin{align*}
		& \sup\limits_{M \in \partial G_2^{\eta}(\eta+d) } \| f(\eta+d,v)-f(\eta)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex} = \| -cv +d +cv -d \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right)  \text{ für } \| d \|_{\h} \rightarrow 0
	\end{align*}	
	Sei nun $d<0$. Es gilt: Es gilt $\partial G_2^{\eta}(\eta+d) = \{0\}$ und damit 	
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta,v)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex}= \| -cv+cv \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right) 
	\end{align*}
	 für $ \| d \|_{\h} \rightarrow 0$. Damit ist $f$ semidifferenzierbar nach $\eta$. Die Semidifferenzierbarkeit von $v$ berechnet man analog.  
\end{proof}

Das eigentliche Ziel war, das Semidifferenzial von $G_2$ zu finden. Dieses können wir nun tun.

\begin{thm}
	$G_2: \h^2 \rightarrow H^{-1}(\Omega)$ mit 
\begin{align*}
			(v, \eta) \mapsto	\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
\end{align*}
ist semidifferenzierbar mit 
	\begin{align*}
		\partial G_{2 \eta}(\eta, v)(\psi_2, \phi_2) = \int\limits_{\Omega} \frac{\partial f}{\partial \eta} \psi_2 \phi_2 \diff x 
	\end{align*}
	\begin{align*}
		\partial G_{2 v}(\eta, v) (\psi_2, \phi_1) = \int\limits_{\Omega}  \frac{\partial f}{\partial v} \psi_2 \phi_1 \diff x 
	\end{align*}
\end{thm}
\begin{proof}
	Es gilt
	\begin{align*}
		 & \int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x \\
		 & = \int\limits_{\Omega} \eta \psi_2 \diff x - \int\limits_{\Omega} \max\{0, \eta + c(v-v_0)\} \psi_2 \diff x - \int\limits_{\Omega} \min\{0, \eta + c v\} \psi_2 \diff x \\
		 & = F_1(v,\eta)(\psi_2) - F_2(v,\eta)(\psi_2) - F_3(v,\eta)(\psi_2)  
	\end{align*}
	Wir können nach \ref{thm:rechenregeln_semidifferenzierbare_fkt} $F_1$, $F_2$, $F_3$ getrennt ableiten. Betrachten wir die Ableitung nach $\eta$. Die Ableitung von $F_1$ ist einfach
	\begin{align*}
		\partial F_{1 \eta}(\psi_2, \phi_2) = \int_{\Omega} \psi_2 \phi_2 \diff x 
	\end{align*}
	$F_2$ und $F_3$ lassen sich mithilfe der Kettenregel aus \ref{thm:rechenregeln_semidifferenzierbare_fkt} ableiten
	\begin{align*}
		\partial F_{2 \eta}(\psi_2, \phi_2) & = \int\limits_{\Omega} \frac{\partial}{\partial \eta + c(v-v_0)} \max\{0, \eta + c(v-v_0)\} \frac{\partial \eta}{\partial \eta} \psi_2 \diff x \\
		& =  \int\limits_{\Omega} \frac{\partial f_2}{\partial \eta} \psi_2 \phi_2 \diff x \\
		\partial F_{3 \eta}(\psi_2, \phi_2) & = \int\limits_{\Omega} \frac{\partial}{\partial \eta + cv} \min\{0, \eta + cv\} \frac{\partial \eta}{\partial \eta} \psi_2 \diff x \\
			& =  \int\limits_{\Omega} \frac{\partial f_3}{\partial \eta} \psi_2 \phi_2 \diff x
	\end{align*}
	mit $f_1$ und $f_2$ der $\min$ bzw. der $\max$ Term. Die Ableitungen kann man davon einfach berechnen. 
	Es ergibt sich: 
	\begin{align*}
		\partial G_{2 \eta}(\eta, v)(\psi_2, \phi_2) & = \partial F_1 (\eta, v)(\psi_2, \phi_2) + \partial F_2 (\eta, v)(\psi_2, \phi_2) + \partial F_3 (\eta, v)(\psi_2, \phi_2) \\
		& =  \int\limits_{\Omega} \psi_2 \phi_2 \diff x + \int\limits_{\Omega} \frac{\partial f_2}{\partial \eta} \psi_2 \phi_2 \diff x \int\limits_{\Omega} \frac{\partial f_3}{\partial \eta} \psi_2 \phi_2 \diff x \\
		& = \int\limits_{\Omega} (id + \frac{\partial f_2}{\partial \eta}  +\frac{\partial f_3}{\partial \eta} ) \phi_2 \psi_2 \diff x \\
		& = \int\limits_{\Omega} \frac{\partial f}{\partial \eta} \psi_2 \phi_2 \diff x 
	\end{align*} 
	Es fügen sich die Ableitungen von $f_2,f_3$ und $\eta$ wieder zu $\frac{\partial f}{\partial \eta} $ zusammen. 
\end{proof}

Damit ergibt sich als Ableitung 
\begin{align*}
	G'(v,\eta)= 
	\begin{pmatrix}
			G_{1 v} & G_{1 \eta} \\
			G_{2 v} & G_{2 \eta}  
	\end{pmatrix}
\end{align*}
Also lautet das Gleichungssystem, das für das Newtonverfahren nach $s$ gelöst werden muss
\begin{align*}
	- \begin{pmatrix}
		G_1 \\
		G_2
	\end{pmatrix}
	= 
	\begin{pmatrix}
			G_{1 v} & G_{1 \eta} \\
			G_{2 v} & G_{2 \eta}  
	\end{pmatrix}
	\begin{pmatrix}
	s^1 \\
	s^2
	\end{pmatrix}
\end{align*}

Stellen wir dazu die Newton Methode auf: 

\begin{algorithm}[H]
	\caption{semidifferenzeirbare Newton Methode}
	\label{algo:semidiffbare_nm_anw}
	\KwData{$v^0, \eta^0 $ möglichst nah an der Lösung $\overline{v}, \overline{\eta}$}
	\For{$k=0,1,\cdots$} {
		\emph{Erhalte $s_1^k$ beim Lösen von $
			 - \begin{pmatrix}
			 G_1 \\
			 G_2
			 \end{pmatrix}
			 = 
			 \begin{pmatrix}
			 G_{1 v} & G_{1 \eta} \\
			 G_{2 v} & G_{2 \eta}  
			 \end{pmatrix}
			 \begin{pmatrix}
			 s^1 \\
			 s^2
			 \end{pmatrix}$}\;
		$v^{k+1}=v^k+s_1^k \hspace{2ex}  \eta^{k+1}=\eta^k+s_2^k$\;
	}
\end{algorithm}

\subsection{Numerische Betrachtung}

Alle Funktionen aus dem Newtonsystem müssen numerisch dargestellt werden. 

Für die Diskretisierung wird das selbe Gitter und die selben Elemente genommen wie bei der Optimierung nach u. Auch hier werden wir wieder mit dem Galerkin Ansatz arbeiten, d.h. 
\begin{align*}
	v=\sum\limits_{i=1}^{k} v_i^h T_i \hspace{9ex} \eta = \sum\limits_{i=1}^{k} \eta_i^h T_i \hspace{9ex} v_0=\sum\limits_{i=1}^{k} {v_0}_i^h T_i
\end{align*}
wobei die $T_i$ wieder die globalen Formfunktionen sind. 

Da $u$ gegeben ist, ist $u$ ein Vektor mit den Auswertungen an den Ecken der Dreiecke. Die Darstellung ist die gleiche wie in \eqref{eq:ungerades_dreieck_lineare_fkt} und \eqref{eq:ungerades_dreieck_lineare_fkt}. Also gilt 
\begin{align*}
	|\nabla u|^2= (u_{31}-u_{21})^2+(u_{11}-u_{21})^2+ (u_{32}-u_{22})^2+(u_{12}-u_{22})^2=:u^{dis}
\end{align*}  

\subsubsection{Numerische Darstellung von $G_{1}$}
Betrachten wir
\begin{align*}
G_1(v,\eta) =  \int\limits_{\Omega} 2 \psi_1 v | \nabla u|^2 + \nu \left(  \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 + \right) \eta \psi_1 \diff x
\end{align*}
Die Diskretisierung lautet: 
\begin{align*}
	& \hspace{2ex} \int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( 2 \epsilon_2  \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1  \right) +  \eta \psi_1  \diff x \\
	& = \int\limits_{\Omega} 2 T_j  \sum\limits_{i=1}^{k} v_i^h T_i u^{dis} + \nu \left( \epsilon_2 2 \sum\limits_{i=1}^{k} v_i^h \nabla T_i \nabla  T_j - \frac{2}{\epsilon_2} (1-\sum\limits_{i=1}^{k} v_i^h T_i) T_j \right)\\
	&  \hspace{3ex} + \sum\limits_{i=1}^{k} \eta_i^h T_i T_j  \diff x \\
	& =  \sum\limits_{i=1}^{k} v_i^h \left(  2 \int\limits_{\Omega} u^{dis} T_i T_j \diff x + 2 \epsilon_2 \nu \int\limits_{\Omega} \nabla T_i \nabla T_j \diff x +  \nu \frac{2}{\epsilon_2}  \int\limits_{\Omega} T_i T_j \diff x  \right) \\
	& - \nu \frac{2}{\epsilon_2} \sum\limits_{i=1}^{k} \int\limits_{\Omega} T_j \diff x 
	+  \sum\limits_{i=1}^{k} \eta_i^h \int\limits_{\Omega} T_i T_j \diff x \\
	& = (2 A + \nu 2 \epsilon_2 B + \nu \frac{2}{\epsilon_2} D) v^h - \nu \frac{2}{\epsilon_2} D*e + D\eta^h
\end{align*}

mit $e:=(1,\cdots,1)^T v^h:=(v_1^h, \cdots v_k^h)^T$, $\eta^h:=(\eta_1^h, \cdots \eta_k^h)^T$, $A_{ij}=  \int_{\Omega} u^{dis} T_i T_j \diff x $, $B_{ij}:= \int_{\Omega} \nabla T_i \nabla T_j \diff x$, $c_j:= \int_{\Omega} T_j \diff x$ und $D_{ij}:= \int_{\Omega} T_i T_j \diff x $.

Die Berechnung der Matrizen $A,B,D$ erfolgt im Anhang \ref{sec:rechnungen}. Es ergibt sich 
\begin{equation*}
	(2 A + 2 \nu \epsilon_2 B + \frac{2 \nu }{\epsilon_2} D) v^h  - \frac{2 \nu }{\epsilon_2} De + D \eta^h =  
\end{equation*}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.15]{images/formel.png}	
	\label{fig:formel}
\end{figure}
%todo BildÄndern

wobei bei $A$ auf der Hauptdiagonalen $\frac{1}{12} ( \sum\limits_{i=1}^6 u^{dis}_{E_i} )$ steht, auf der Nebendiagonalen steht $\frac{1}{24} ( u^{dis}_{E_3} + u^{dis}_{E_6} ) $ , auf der zweiten Nebendiagonalen $ \frac{1}{24} ( u^{dis}_{E_4} + u^{dis}_{E_5} ) $  und auf der dritten Nebendiagonalen $ \frac{1}{24} ( u^{dis}_{E_5} +u^{dis}_{E_6} )$. 

Bei $B$ steht  auf der Hauptdiagonalen $4 $, auf der Nebendiagonalen und der zweiten Nebendiagonalen  $-1 $. 

Bei $D$ steht  auf der Hauptdiagonalen $\frac{1}{2} $, auf der ersten, zweiten und dritten Nebendiagonalen $\frac{1}{12} $. 

Durch die noch ausstehende Transformation der Dreiecke, muss der gesamte Term mit $1/ h_1 h_2$ multipliziert werden.  

\subsubsection{Numerische Darstellung von $G_{2}$}
\begin{align*}
	\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
\end{align*}
Um dieses Funktional numerisch darzustellen, benutzen wir den Galerkin Ansatz mit 
\begin{align*}
	& v = \sum\limits_{i=1}^k v_i^h T_i(x,y) \hspace{9ex} \eta = \sum\limits_{i=1}^k \eta_i^h T_i(x,y) \hspace{9ex} v_0 = \sum\limits_{i=1}^k {v_0}_i^h T_i(x,y) 
\end{align*}
Daraus ergibt sich:
\begin{align*}
	& \int\limits_{\Omega} \left( \eta - \max \left\{ 0, \eta + c(v-v_0) \right\} - \min \left\{ 0, \eta + c v \right\} \right) \psi_2 \diff x  \\
	& = \int\limits_{\Omega} \left(  \sum\limits_{i=1}^k \eta_i^h T_i \right. - \max \left\{ 0,  \sum\limits_{i=1}^k \eta_i^h T_i + c(\sum\limits_{i=1}^k v_i^h T_i - \sum\limits_{i=1}^k {v_0}_i^h T_i ) \right\} \\
	& \hspace{5ex} \left. - \min \left\{ 0,  \sum\limits_{i=1}^k \eta_i^h T_i + c \sum\limits_{i=1}^k v_i^h T_i \right\} \right) T_j \diff x  \\ 
	& = \int\limits_{\Omega} \left(  \sum\limits_{i=1}^k \eta_i^h T_i \right. -   \sum\limits_{i=1}^k \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h )  \right\} T_i \\
	& \hspace{5ex} \left. -\sum\limits_{i=1}^k  \min \left\{ 0,  \eta_i^h + c v_i^h    \right\} T_i  \right) T_j \diff x  \\ 
	& = \int\limits_{\Omega}  \sum\limits_{i=1}^k \left(  \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  \right) T_i T_j \diff x \\
	& =  \left( \sum\limits_{i=1}^k   \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  \right) \int\limits_{\Omega} T_i T_j \diff x \\	  
	& = D w_{v \eta}	
\end{align*}
mit $D$ aus der numerischen Darstellung von $G_1$ und\\ $(w_{v \eta})_i:=  \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}$. Die Summe und $T_i$ dürfen aus dem $\max$ bzw. $\min$ herausgezogen werden, da $T_i$ immer nur an einem Punkt ungleich 0 ist. Dadurch kommt niemals zustande, dass mehr als ein Term der Summe ungleich 0 ist. 
$w_{v \eta}	$ kann auch explizit dargestellt werden: 
\begin{align*}
	(w_{v \eta})_i = 
		\left\{
			\begin{array}{ll}
				- c(v_i^h - {v_0}_i^h ) 	& \text{ falls }  -c(v_i^h - {v_0}_i^h ) \le \eta_i^h \\
				\eta_i^h 		& \text{ falls } -c v_i^h  < \eta < -c(v_i^h - {v_0}_i^h ) \\
				-c v_i^h	& \text{ falls }  \eta_i^h \le -c v_i^h
			\end{array}
		\right.
\end{align*}
Auch hier muss das Integral wieder transformiert werden, wodurch der Faktor $1/h_1 h_2$ multipliziert wird. 

\subsubsection{Numerische Darstellung von $G_{1 v}$}
\begin{align*}
 	G_{1 v} (v, \eta) = \int\limits_{\Omega} 2  \psi_1  \phi_1 | \nabla u|^2 + \nu 2 \epsilon_2 \nabla \phi_1 \nabla \psi_1  + \frac{2 \nu}{\epsilon_2} \phi_1 \psi_1 \diff x
\end{align*}

wird nun diskretisieren:

\begin{align*}
	& \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + 2 \nu \epsilon_2  \nabla \phi_1 \nabla \psi_1  + \frac{2 \nu}{\epsilon_2} \phi_1 \psi_1 \diff x\\
	& = \int\limits_{\Omega} 2  T_j  T_i u^{dis} + 2 \nu \epsilon_2  \nabla  T_i  \nabla  T_j  + \frac{2 \nu}{\epsilon_2}  T_i T_j \diff x \\
	& = 2 A  + 2 \nu \epsilon_2  B + \frac{2 \nu}{\epsilon_2}  D 
\end{align*}

Wir benutzen die gleichen Notationen, wie bei der numerischen Darstellung von $G_1$ und die gleiche Transformation. 

\subsubsection{Numerische Darstellung von $G_{1 \eta}$}
\begin{align*}
 	G_{1 \eta} (v, \eta) = \int\limits_{\Omega}  \phi_2 \psi_1  \diff x
\end{align*}

wird nun diskretisieren:
\begin{align*}
	& \int\limits_{\Omega}  \phi_2 \psi_1 \diff x = \int\limits_{\Omega} T_j  T_i =  D 
\end{align*}

Auch hier muss wieder transformiert werden. 

\subsubsection{Numerische Darstellung von $G_{2 v}$}
Es soll 
	\begin{align*}
		\partial G_{2 v}(\eta, v) (\psi_2, \phi_1) = \int\limits_{\Omega}  \frac{\partial f}{\partial v} \psi_2 \phi_1 \diff x 
	\end{align*}
mit 
	\begin{align*}
		\frac{\partial f}{\partial v}= 
		\left\{
		\begin{array}{ll}
			\{-c\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{0\}		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack -c,0 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right..
	\end{align*}
numerisch dargestellt werden. Statt $\frac{\partial f}{\partial v}$ implementieren wir eine Vereinfachung, die nicht mehr Mengenwertig ist. Dazu wählen wir statt $\lbrack -c,0 \rbrack$ einen Punkt aus dem Intervall, z.B. $-c/2$. Nun kann  $\frac{\partial f}{\partial v}$  diskretisiert werden zu $f^h$. Dies ist einfach die Funktion ausgewertet an den Gitterpunkten.

Nun wird $\partial G_{2 v}(\eta, v) (\psi_2, \phi_1)$ diskretisiert. Hier wird wie immer $\psi_2, \phi_1$ durch die globalen Formfunktionen $T_i$ ersetzt und $\Omega $ durch die Vereinigung aller Dreiecke. Nun kann für jedes Dreieck  $\int\limits_E  \frac{\partial f}{\partial v} T_i T_j \diff x $ berechnet werden. Dies erfolgt im Anhang \ref{sec:rechnungen}. 
 
Wir erhalten wieder eine Matrix folgender Form:

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{images/matrix2.png}
	\caption{Darstellung der Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ }
	\label{fig:int_g2v}
\end{figure}
%todo matrix erstellen. 
Jeder Eintrag, der nicht 0 ist, besteht aus der Summe von unterschiedlichen Auswertungen der Funktion $f$ auf den Dreiecken, über denen integriert wurde. 

\subsubsection{Numerische Darstellung von $G_{2 \eta}$}
Die numerische Darstellung ist genau die gleiche, wie bei $G_{2 v}$, nur dass die Funktionsauswertungen von $f$ andere sind. 

\subsection{Aggregation}
Nun sind alle Funktionen diskretisiert und das Problem kann implementiert werden. Stellen wir die genaue Newton Methode auf. 

\begin{algorithm}[H]
	\caption{semidifferenzierbare Newton Methode}
	\label{algo:semidiffbare_nm_anw}
	\KwData{$v^0, \eta^h $ (möglichst nah an der Lösung $(\overline{v},\overline{\eta})$)}
	\For{$k=0,1,\cdots$} {
		\emph{Löse das Gleichungssytem $L u^k = L u_0^k$ nach $u^k$ }\;
		\emph{Erhalte $s_k$ beim Lösen von $ -\begin{pmatrix}
			G_1(v^k,\eta^k) \\ G_2(v^k,\eta^k)
			\end{pmatrix} = \begin{pmatrix}
				\partial G_{1v}(v^k, \eta^k)(T_i) & \partial G_{1\eta}(v^k, \eta^k)(T_i) \\
				\partial G_{2v}(v^k, \eta^k)(T_i) & \partial G_{2 \eta}(v^k, \eta^k)(T_i) \\  
			\end{pmatrix} 
			\begin{pmatrix}
			s^k_1 \\ s^k_2_
			\end{pmatrix}$}\;
		$v^{k+1}=v^k+s_1^k \hspace{2ex} \eta^{k+1}= \eta^k + s_2^k$\;
	}
\end{algorithm}
mit
\begin{align*}
	\begin{array}{rcl}
		L 					& = & \int\limits_{\Omega} (v^2 + \epsilon_1) \nabla T_i \nabla T_j \diff x \\
		G_1 				& = &  \left(2 (\int\limits_{\Omega} u^{dis} T_i T_j \diff x)_{ij} + 2 \nu \epsilon_2 (\int\limits_{\Omega} \nabla T_i \nabla T_j \diff x)_{ij} +  \frac{2 \nu}{\epsilon_2}  (\int\limits_{\Omega} T_i T_j \diff x)_{ij}  \right) v^k \\
		& & - \frac{2 \nu}{\epsilon_2} (\int\limits_{\Omega} T_j \diff x)_j +  ( \int\limits_{\Omega} T_i T_j \diff x )_{ij} \eta^k \\
		G_2 				& = &   ( \int\limits_{\Omega} T_i T_j \diff x)_{ij}     (\eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  )_i\\
		\partial G_{1v} 	& = & 2 (\int\limits_{\Omega} u^{dis} T_i T_j \diff x)_{ij} + 2 \nu \epsilon_2 (\int\limits_{\Omega} \nabla T_i \nabla T_j \diff x)_{ij} +  \frac{2 \nu}{\epsilon_2}  (\int\limits_{\Omega} T_i T_j \diff x)_{ij}\\
		\partial G_{1\eta}	& = & (\int\limits_{\Omega} T_i T_j \diff x)_{ij}\\
		\partial G_{2v} 	& = & (\int\limits_{\Omega} \frac{\partial f}{\partial v} T_i T_j)_{ij} \\
		\partial G_{2 \eta} & = & (\int\limits_{\Omega} \frac{\partial f}{\partial \eta} T_i T_j)_{ij} \\
		f					& = &  \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\}				
	\end{array}
\end{align*}
