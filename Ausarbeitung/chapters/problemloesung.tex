% ==============
% Anwendung auf das Phasenfeldmodell für Rissentstehung 
% ==============

\chapter{Anwendung auf das Phasenfeldmodell für Rissentstehung }

Nachdem wir die mathematischen Grundlagen für die Betrachtung eines Optimierungsproblems kennengelernt haben, wollen wir diese anwenden. 
Zunächst teilen wir das Minimierungsproblem in zwei voneinander unabhängige Optimierungen auf: Die Optimierung nach $u$ und die Optimierung nach $v$. Im Anschluss betrachten wir beide Optimierungen genauer, indem wir sie umschreiben und numerische Verfahren zur Lösung entwickeln. Zum Schluss fusionieren wir beide Verfahren.


\section{Erste Betrachtung des Modells}
\label{sec:allgemeines_kkt}

Erinnern wir uns an die vorangegangene Problemstellung 
\begin{align*}
	& \min\limits_{u \in  \h^2 , v \in \h } \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 \\\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2  
\end{align*}

Beim genaueren Betrachten bemerkt man, dass die Ungleichungsnebenbedingung nur von $v$ und die Randbedingung nur von $u$ abhängt. Dies bietet die Möglichkeit das Optimierungsproblem in zwei Teilprobleme aufzuteilen.
\begin{align*}
\label{eq:problem_von_v}
	& \min\limits_{u \in\h^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2 
	\vspace{1ex} \\
	& \min\limits_{v \in\h} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 
\end{align*}

Wenn man beide Probleme implementiert, löst man zunächst die Optimierung nach $u$ und setzt die Lösung dann in die Optimierung nach $v$ ein. Danach setzt man die Lösung von $v$ in die Optimierung nach $u$ ein. Dieses Vorgehen wird mit der semidifferenzierbaren Newtonmethode wiederholt. Betrachten wir zuerst die Optimierung nach u. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Optimierung nach u          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Optimierung nach u}

Das Kapitel ist in zwei Unterkapitel aufgeteilt: Die analytische und numerische Betrachtung. 

Im analytischen Teil formulieren wir das Problem zu einem Problem ohne Nebenbedingung um. Dieses lässt sich dann als partielle Differentialgleichung schreiben. Die Existenz und Eindeutigkeit der schwachen Lösung sichern wir uns am Schluss des ersten Teils. 

Für die numerische Betrachtung schreiben wir das Problem nochmal um und wenden dann Finite Elemente und den Galerkin Ansatz an. Das Resultat ist ein Gleichungssytem, das sich einfach lösen lässt. 

\subsection{Analytische Betrachtung}

Erinnern wir uns an das Optimierungsproblem, das die Verschiebung des Körpers bei der Entstehung von einem Riss beschreibt.
\begin{align*}
	& \min\limits_{u \in\h^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& u=u_0 \text{ auf } \Gamma_1 \cup \Gamma_2
\end{align*}

Es ist leichter ein Problem ohne Nebenbedingung zu betrachten, also nehmen wir die Nebenbedingung mit in dem Raum auf, über den wir optimieren. Also suchen wir statt $u \in \h^2$ 
\begin{align*}
	u \in u_0 + \ho^2:=u_0+\{u \in\h^2| u=0 \text{ auf } \Gamma_1 \cup \Gamma_2\} 
\end{align*}
Das Problem hat dann folgende Form
\begin{align}
	\label{eq:optimierung_u_ohne_nb2}
	& \min\limits_{u \in u_0 +\ho^2} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x 
\end{align}

Da $u: \Omega \rightarrow \R^2$, müssen wir nach $u_1$ und nach $u_2$ minimieren. Es tauchen keine Mischung aus den Termen $u_1$ und $u_2$ auf, das heißt, dass wir die Optimierungen trennen können. Beide sind identisch, es müssen später nur unterschiedliche Werte eingesetzt werden. Betrachten wir oBdA die Optimierung nach $u_1$. 

\begin{thm}[Bedingung für ein Minimum]
	Sei das Minimierungsproblem \eqref{eq:optimierung_u_ohne_nb2} gegeben und $\tilde{u_1}$  nimmt das Minimum an. Dann gilt 
	\begin{align}
		\label{eq:u_gleich_null}
		\int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right)  \nabla \tilde{u_1} \nabla  \psi \diff x = 0 \forall  \psi \in u_0 + \ho 
	\end{align}
\end{thm}
\begin{proof}
	Nach \ref{thm:ableitung_gleich_null} muss nur überprüft werden, ob die Gâteaux-Ableitung von 
	\begin{align*}
		\begin{array}{rrcl}
			J: 	& u_0+ \ho^2& \rightarrow	& \R \\
			& u 											& \mapsto		& \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x 
		\end{array}
	\end{align*}
	\eqref{eq:u_gleich_null} ist. Leiten wir $J$ ab: 
	\begin{align*}
		\begin{array}{lcrl}
			\partial J(u)( \psi ) & = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big( & J(u_1+t  \psi,u_2)-J(u_1,u_2) \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla (u_1+t  \psi )|^2 + |\nabla u_2|^2 + \epsilon_2 |\nabla v|^2  \\ 
			& & &\hspace{-2ex} + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x \\
			& & &\hspace{-2ex} - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla u_1|^2 \hspace{7ex} + |\nabla u_2|^2  + \epsilon_2 |\nabla v|^2  \\
			& & &\hspace{-2ex} + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \diff x \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \left( | \nabla (u_1+t  \psi )|^2 - | \nabla u_1|^2 \right) \diff x  \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( | \nabla u_1+t \nabla  \psi |^2 - | \nabla u_1|^2 ) \diff x  
			 \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( | \nabla u_1|^2 + 2 t  \nabla u_1 \nabla  \psi  + t^2 |\nabla  \psi |^2 - | \nabla u_1|^2 ) \diff x   \\					
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 	& 
			\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) ( 2 t  \nabla u_1 \nabla  \psi  + t^2 |\nabla  \psi |^2 ) \diff x   \\	
			& = & &\int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right) \nabla u_1 \nabla  \psi \diff x  
		\end{array}	
	\end{align*}
	Damit dies eine G\^ateaux Ableitung ist, muss die Abbildung $J'(u_1):  \psi  \mapsto \partial J(u_1, \psi ) \in \R $ linear und beschränkt sein. Linearität ist einfach nachzurechnen. Beschränktheit lässt sich durch Cauchy-Schwarz zeigen.   
\end{proof}

Also lautet unser analytisches Problem:
Finde $u_1 \in u_0+\ho$, sodass  $\forall  \psi  \in u_0 +\ho$ gilt 
\begin{align}
\label{eq:problem_u}
	0 = \int\limits_{\Omega} 2 \left( v^2 + \epsilon_1 \right)  \nabla u_1 \nabla  \psi   \diff x 
\end{align}
Nun ist noch interessant, ob eine Lösung existiert und ob sie eindeutig ist. Dieses hängt von $u_0$ und $v_0$ ab. 

\begin{thm}[Existenz und Eindeutigkeit]
	Sei $u_0 \in \h^2, ,v_0 \in \h$. Die schwache Lösung $u \in u_0+\ho$ von \eqref{eq:problem_u} existiert und ist eindeutig. 
\end{thm}
\begin{proof}
Wir wenden \ref{thm:existenz_schwache_loesung} an. Dazu müssen wir die Bilinearform aufstellen und dann \ref{ann:ex_und_eind} zeigen. 
Die Bilinearform lautet 
\begin{align*}
	\begin{array}{rrcl}
		B(u_1, \psi ): 	& \left( u_0 +\h0^2 \right)^2  & \rightarrow 	& \R \\
		& (u_1, \psi )											& \mapsto		& \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) 2  \nabla u_1 \nabla  \psi   \diff x 
	\end{array}
\end{align*}
Mit den Bezeichnungen aus \ref{sec:grundlagen_pdgl} ist $g = u_0$, $f,b,c=0$ und  
\begin{align*}
	A(x):= \begin{pmatrix}
	v^2(x) + \epsilon_1 & 0 \\
	0 & v^2(x) + \epsilon_1 
	\end{pmatrix}		
\end{align*}
Aus \ref{ann:ex_und_eind} sind 3 und 4 bereits erfüllt, da $b,c=0$ gilt. Beweisen wir 1. 

Sei $\xi \in \R^n$. Dann gilt:
\begin{align*}
	\xi^T A(x) \xi  & = \xi^T  \begin{pmatrix}
	v^2(x) + \epsilon_1 & 0 \\
	0 & v^2(x) + \epsilon_1 
	\end{pmatrix}		 \\
	& = (v^2+ \epsilon_1) \xi \cdot \xi \\
	& \ge \epsilon_1 |\xi|^2
\end{align*}
Damit ist Annahme 1 erfüllt mit $\lambda = \epsilon_1$. 
Für Annahme 2 gilt
\begin{align*}
	|\xi^T A(x) \zeta| =  (v^2 + \epsilon_1) \xi \cdot \zeta   \le (v_0^2 + \epsilon_2) \xi \cdot \zeta 
     \le (\sup (v_0)^2 + \epsilon_2)  |\xi| |\zeta| 
\end{align*}
mit $\Lambda = \sup (v_0)^2 + \epsilon   $
\end{proof}

\subsection{Numerische Betrachtung}

Wir haben grade bewiesen, dass wir folgendes Problem lösen müssen:

Finde $u_1 \in u_0+ H_0^1(\Omega)$, sodass
\begin{align*}
	0 = \int\limits_{\Omega} \left( v^2 + \epsilon_1 \right) \nabla u_1 \nabla  \psi \diff x \hspace{2ex} \forall  \psi \in u_0 + H_0^1(\Omega)
\end{align*}

Da die Nullstelle im Raum $H_0^1(\Omega)$ einfacher zu finden ist, als im Raum $u_0 + H_0^1(\Omega)$, stellen wir das Problem um. 
Dazu definieren wir $\uotild \in u_0 + \ho$, sodass $\uotild$ $u_{0_1}$ auf dem Rand $\Gamma_1 \cup \Gamma_2$ entspricht und sonst $0$ ist. Definiere zusätzlich $\utild \in \ho$, sodass $u_1=\utild + \uotild$. 
Damit lässt sich das Problem umschreiben zu 

Finde $\utild \in \ho$, sodass  $\forall  \psi \in \ho$  
\begin{align*}
	- \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla \uotild \nabla  \psi \diff x  = \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla \utild \nabla  \psi \diff x 
\end{align*}

Zur numerischen Betrachtung bieten sich Finite Elemente, insbesondere die dreieckig linearen Lagrange Elemente an. Dafür triangulieren wir das Gebiet, wie in \ref{sec:finite_elemente} dargestellt. Nun nutzen wir den Galerkin Ansatz. Dafür gilt ab jetzt $k:=(m+1)(n+1)$
\begin{align*}
	\utild(x,y):= \sum\limits_{i=1}^{k} u^h_i T_i(x,y) 
\end{align*}

Dabei sind  $T_i(x,y)$ die globalen Formfunktionen und $u^h_i$ die gesuchten Konstanten. Setzt man die Definition von $\utild$ ein und ersetzt $ \psi \in \ho$ durch die Basis von $P^*$, also den globalen Formfunktionen $T_i$, so gilt $\forall i \in \{1, \cdots , k\}$  
\begin{align*}
	& - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla \uotild \nabla  \psi \diff x  = \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla \utild \nabla  \psi \diff x \\
	\Leftrightarrow 
	& - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \sum\limits_{i=1}^{k} {u^h_0}_i \nabla T_i \nabla T_i \diff x  =\int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \sum\limits_{i=1}^{k} u^h_i \nabla T_i  \nabla T_j  \diff x \\
	\Leftrightarrow
	& - \sum\limits_{i=1}^{k} {u^h_0}_i \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)  \nabla T_j \nabla T_i \diff x
	= \sum\limits_{i=1}^{k} u^h_i \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right)   \nabla T_i \nabla T_j  \diff x \\
	\Leftrightarrow
	& L*u_0^h =  L * u^h
\end{align*}
wobei $u^h:= (u^h_1, \cdots u^h_{k})^T $, $u_0^h:= ({u^h_0}_1, \cdots {u^h_0}_k)^T $ und $L:= \left( \int_{\Omega}  \left( v^2 + \epsilon \right)   \nabla T_i \nabla T_j  \diff x \right)_{ij} $

Also müssen wir $L$ berechnen und dann das Gleichungssystem $ L*u_0^h =  L * u^h$ lösen. 

\subsubsection{Berechnung des $u$ Integrals} 
Als erste Vereinfacherung betrachten wir nicht mehr das Integral über $\Omega$, sondern über die einzelnen Dreiecke der Triangulierung. Desweiteren ist $T_i$ linear, also $\nabla T_i$ konstant. Es gilt  
\begin{align*}
  \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) \nabla T_i \nabla T_j  \diff x  
 & = \sum\limits_{\tilde{E} \in E_k} \int\limits_{\tilde{E}}  \left( v^2 + \epsilon_1 \right) \nabla T_i \nabla T_j  \diff x \\
 & =  \sum\limits_{\tilde{E} \in E_k} \nabla T_i \nabla T_j \int\limits_{\tilde{E}}  \left( v^2 + \epsilon_1 \right)   \diff x 
\end{align*}

Wir kennen $\nabla T_i \nabla T_j$ auf jedem Dreieck. Also muss nur noch $\int_E v^2+ \epsilon \diff x$ berechnet werden. Es darf über das Referenzdreieck integriert werden, da durch den Transformationssatz das Integral über das transformierte Element gewonnen werden kann. Es gilt:
\begin{align*}
	\int\limits_E v^2 + \epsilon_1 \diff x &  = \int\limits_E v^2  \diff x + \frac{1}{2}\epsilon_1  \\
\end{align*}
Da $v$ bereits numerisch berechnet wurde, haben wir nur Funktionsauswertungen von $v$ an den Ecken des Dreieckes gegeben und wir wissen, dass $v \in \mathcal{P}_1$. Also ist v eindeutig bestimmt und kann berechnet werden. Die Darstellung von $v$ ist in \eqref{eq:gerades_dreieck_lineare_fkt} und \eqref{eq:ungerades_dreieck_lineare_fkt} zu finden. 

Die Berechnung von $\int_E v^2 \diff x$ sieht wie folgt aus
\begin{align*}
	\int\limits_E v(x,y)^2 \diff x \diff y 
	& = \int\limits_0^1 \int\limits_0^{1-y}
	 \left( 
	 	(v_3-v_1)x+(v_2-v_1)y+v_1
	 \right)^2
	 \diff x \diff y \\ 
	 & = \frac{1}{12} (v_1^2+v_2^2+v_3^2 + v_1v_2 + v_1 v_3 + v_2 v_3)
\end{align*}
Da die Berechnung über das transformierte Element durchgeführt wurde, muss noch der Multiplikator $\frac{1}{h_1 h_2}$ eingefügt werden. 

Berechnen wir nun 
\begin{align*}
L_{i,j} = \sum\limits_{\tilde{E} \in E_k} \nabla T_i \nabla T_j \int\limits_{\tilde{E}}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}

$T_i$ ist nur auf dem Gitterpunkt $i$ $1$ und sonst $0$. Das heißt genauer, dass $T_i$ nur auf sechs Dreiecken ungleich 0 ist. Um das Integral zu bestimmen braucht man also maximal sechs Dreiecke. Falls der Gitterpunkt am Rand liegen sollte, betrachtet man nur drei Dreiecke, an den Ecken entweder ein oder zwei Dreiecke. 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/triang_inner.png}
	\caption{Triangulierung im Inneren}
	\label{fig:triang_inner}
\end{figure}

Jetzt können wir $L_{ij}$ für festes $i,j$ berechnen. Falls $i$ und $j$ nicht adjazent sind, ist $L_{ij}=0$, da $T_i T_j=0$ gilt. Seien nun $T_I$ und $T_j$ adjazent. Hier haben wir vier Fälle:
\begin{itemize}
	\item  $i$ liegt auf $j$, also $j=i$ 
	\item  $j$ liegt rechts neben $i$ also $j=i+1$
	\item $j$ liegt direkt unter $i$, $j=i+n+1$ 
	\item  $j$ liegt schräg unter $i$, also $j=i+n+2$
\end{itemize}
Betrachten wir für die einzelnen Berechnungen \ref{fig:triang_inner}. $T_i$ ist immer der Mittelpunkt dieser Zeichnung, $T_j$ ist entsprechend des jeweiligen $j$ positioniert. In den Berechnungen stimmen die Nummerierungen der Dreiecke mit den Nummerierungen in der Abbildung \ref{fig:triang_inner} überein und $\varphi_k$, $k \in \{1,2,3\}$ entspricht den $\varphi_k$ in \eqref{eq:varphi}. 

Betrachten wir nun die vier Fälle. 

\paragraph*{$i$ und $j$ sind gleich}
\begin{align*}
	L_{i,i}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_i   \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x\\
		& =  \nabla \varphi_1 \nabla \varphi_1 \int\limits_{E_1}  \left( v^2 + \epsilon \right)   \diff x
		+  \nabla \varphi_2 \nabla \varphi_2  \int\limits_{E_2}  \left( v^2 + \epsilon \right)  \diff x\\
		& +  \nabla \varphi_0 \nabla \varphi_0 \int\limits_{E_3}  \left( v^2 + \epsilon \right)   \diff x 
		+ \nabla \varphi_0 \nabla \varphi_0 \int\limits_{E_4} \left( v^2 + \epsilon \right) \diff x \\ 
		& +  \nabla \varphi_2  \nabla \varphi_2  \int\limits_{E_5}  \left( v^2 + \epsilon \right) \diff x	
		+ \nabla \varphi_1 \nabla \varphi_1 \int\limits_{E_6} \left( v^2 + \epsilon \right) \diff x 
\end{align*}
%todo falls zeit reihenfolge anders
\paragraph*{$j$ liegt rechts neben $i$}
\begin{align*}
	L_{i,i+1}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+1}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_3}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_6}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}

\paragraph*{$j$ liegt unter $i$}
\begin{align*}
%korrekte varphi?
	L_{i,i+1+n}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+1+n}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_0 \nabla \varphi_1 \int\limits_{E_4}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_0 \nabla \varphi_1  \int\limits_{E_5}  \left( v^2 + \epsilon \right)   \diff x 
\end{align*}

\paragraph*{$j$ liegt schräg unter $i$}
\begin{align*}
%korrekte varphi?
	L_{i,i+2+n}	& = \sum\limits_{E \in E_k}  \nabla T_i \nabla T_{i+2+n}  \int\limits_{E}  \left( v^2 + \epsilon \right)   \diff x \\
 				& =  \nabla \varphi_1 \nabla \varphi_2 \int\limits_{E_5}  \left( v^2 + \epsilon \right)   \diff x + \nabla \varphi_1\nabla \varphi_2  \int\limits_{E_6}  \left( v^2 + \epsilon \right)   \diff x \\
 				& = 0
\end{align*}

\paragraph*{Zusammenfassung}
Mit diesen Werten können wir die Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ aufstellen:

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/matrix.png}
	\caption{Darstellung der Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ }
	\label{fig:int_u}
\end{figure}
Dabei sind auf der Diagonalen die Einträge $L_{i,i}$, auf der Nebendiagonalen die Einträge $L_{i,i+1}$ und auf der anderen Diagonale die Einträge $L_{i,i+n+1}$. Wir haben bis jetzt immer nur über das Referenzdreieck integriert. Da wir aber eigentlich über die transformierten Dreiecke integrieren, müssen wir zu der Matrix $1/(h_1 h_2)$ multiplizieren. 

\subsection{Aggregation}

Nun haben wir die Matrix $L$ und den Vektor $u_0^h$ gegeben, um das Gleichungssystem $L u^h = L u_0^h$ zu berechnen. Allerdings haben wir noch nicht eingebracht, dass auf $\Gamma_1 \cup \Gamma_2$ $u=0$ gilt. Eigentlich würden wir das im Vektor $u$ mit aufnehmen, also die Zeilen 0 setzten, die den Rand repräsentieren und dann das Gleichungssystem lösen. Dies geht numerisch jedoch nicht so einfach. Die Information muss in $L$ und in $L u_0^h$ codiert sein. Dazu setzt man die Zeilen in $L$ 0, die zum Rand gehören. Die zugehörigen Diagonaleinträge werden 1 gesetzt. Diese Matrix nennen wir $\tilde{L}$. Die zugehörige Einträge in $L u_0^h$, also die Einträge, die in der selben Zeile sind, setzt man 0. Den neuen Vektor nennen wir $\tilde{L u_0^h}$ Dadurch erhält man, dass $u^h$ an dieser Stelle 0 wird. 


Damit haben wir beide Seiten diskretisiert und können das Gleichungssystem implementieren.  
Wir wollen
\begin{align*}
	\frac{1}{h_1 h_2}\tilde{L} u=\frac{1}{h_1 h_2} \tilde{L u_0^h} \Leftrightarrow \tilde{L}u = \tilde{L u_0^h}
\end{align*}
berechnen. Der Code dazu hat folgende Form 

\begin{algorithm}[H]
	\caption{Berechnung von u}
	1. Berechne Matrix $\tilde{L}$ \\
	2. Berechne Vektor $\tilde{L u_0^h}$ \\
	3. $u = \tilde{L u_0^h}\backslash \tilde{L}$ 
\end{algorithm}
Da sowohl $\tilde{L}$ als auch $\tilde{L u_0^h}$ aus fast nur Nullen besteht, verwende ich in Matlab Sparse Matrizen. Dies führt zu einer wesentlich besseren Laufzeit. 

\section{Optimierung nach v}

Bei der Optimierung nach $v$ geht es um die Fortsetzung des Risses. Für dieses Optimierungsproblem mit Ungleichungsnebenbedingung sicheren wir zunächst die Existenz und Eindeutigkeit der Lösung. Danach stellen wir die Optimalitätsbedingungen auf. Das resultiert in ein Karush Kuhn Tucker System. Dieses wollen wir mittels semidifferenzierbarer Newton Methode lösen. Dazu müssen zunächst alle Funktionen der KKT Systems differenziert und danach diskretisiert werden. Dies geschieht wieder mit Finiten Elementen. Am Schluss führe beide Optimierungsprobleme zu einem Verfahren zusammen.    


\subsection{Analytische Betrachtung}
Die Optimierung nach $v$ hat folgende Form:

\begin{align}
	\label{eq:problem_von_v}
	& \min\limits_{v \in\h} \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
	& \text{s.d.} \hspace{1ex} 0 \le v \le v_0 \notag
\end{align}
Das lässt sich allgemein als Optimierungsproblem mit Ungleichungsnebenbedingungen darstellen  
\begin{align*}
 \min\limits_{w \in W} J(w) \quad \text{s.d.} \quad w \in C 
\end{align*}
wobei W ein Banachraum, $J: W \rightarrow \R $ G-diffbar und $C \subset W$. 
In diesem Fall bedeutet das also, dass 
\begin{align*}
	\begin{array}{lrcl}
		J: 	& H^1(\Omega)	& \rightarrow 	& \R \\
			& v				& \mapsto		& \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
		\multicolumn{4}{l}{ C:= \left\{ v \in H^1(\Omega) | 0 \le v \le v_0 \right\} }	
	\end{array}
\end{align*}
Zunächst wollen wir die Existenz und Eindeutigkeit der Lösung zeigen. Dafür brauchen wir, dass $J$ Gâteaux differenzierbar ist. 
\begin{lem}
	\label{lem:J_g_diffbar}
	$J$ ist Gâteaux differenzierbar mit 
	\begin{align*}
		\begin{array}{lrcl}
			J'(v): 	& \overline{H^1}(\Omega) 	& \rightarrow 	& \R \\
			&\psi_1							& \mapsto		&  \int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x\\
		\end{array}	
	\end{align*}
\end{lem}
\begin{proof}
	Zunächst müssen wir die Richtungsableitung bestimmen.   
	\begin{align*}
		\begin{array}{lcrl}
			\partial J(v)(\psi_1) & = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big( & J(v+t\psi_1)-J(v) \Big) \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( (v+t\psi_1)^2 + \epsilon_1 \right) | \nabla (u)|^2 \\ 
			& & &\hspace{-2ex} + \nu \left( \epsilon_2 |\nabla (v+t\psi_1)|^2 + \frac{1}{\epsilon_2} \left( 1- (v+t\psi_1) \right)^2 \right) \\
			& & &\hspace{-2ex} - \int\limits_{\Omega}  \left( v^2 + \epsilon_1 \right) | \nabla u|^2 \hspace{8ex} \\
			& & &\hspace{-2ex} + \nu \left( \epsilon_2 |\nabla v|^2 \hspace{8ex}+ \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \Big)  \\
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega} \left( \left( (v+t\psi_1)^2 + \epsilon_1 \right) - \left( v^2 + \epsilon_1 \right) \right) | \nabla (u)|^2 \\
			& & & \hspace{2ex} + \nu \left( \epsilon_2 \left( |\nabla (v+t\psi_1)|^2 - |\nabla v|^2 \right) \right. \\
			& & & \hspace{2ex} \left. + \frac{1}{\epsilon_2} \left( (1- v- t\psi_1)^2 -  (1- v)^2 \right) \right)	\Big) \\	
			& = &\lim\limits_{t \rightarrow 0} \frac{1}{t} 
			\Big(	& 
			\int\limits_{\Omega}  \left( v^2 + 2vt\psi_1 + t^2\psi_1^2 - v^2  \right) | \nabla u|^2 \\
			& & & \hspace{2ex} + \nu \left( \epsilon_2 \left( |\nabla v|^2 + 2 t \nabla v \nabla \psi_1 + t^2 |\nabla \psi_1|^2 - |\nabla v|^2 \right) \right. \\
			& & & \left. \hspace{2ex} + \frac{1}{\epsilon_2} \left( (1- v)^2 - 2(1-v)t\psi_1 + t^2\psi_1^2 -  (1- v)^2 \right) \right)
			\Big) \\		
			& = &\lim\limits_{t \rightarrow 0} 
			\Big(	& 
			\int\limits_{\Omega}  \left( 2v\psi_1 + t\psi_1^2  \right) | \nabla u|^2 + \nu \left( \epsilon_2 \left( 2 \nabla v \nabla \psi_1 + t |\nabla \psi_1|^2 \right) \right.\\
			& & & \hspace{2ex} \left.  - \frac{1}{\epsilon_2} \left(2(1-v)\psi_1 + t\psi_1^2 \right) \right) \diff x
			\Big) \\	
			& = &\hspace{6.5ex} & 
			\int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x\\	
			& = &\hspace{6.5ex} & 
			\int\limits_{\Omega} 2  v | \nabla u|^2 \psi_1 - \nu \left( \epsilon_2 2 \Delta v  \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 \right) \diff x 	\\
			& & &\hspace{-2ex} + \int\limits_{\partial \Omega} 2 \nu  \epsilon_2 \nabla v \nu \psi_1 \diff x  						 
		\end{array}
	\end{align*}
	Damit es auch eine G\^ateaux Ableitung ist, muss sie beschränkt und linear sein. Dies ist einfach zu sehen. 
\end{proof}

\begin{thm}
Das Problem  \eqref{eq:problem_von_v} besitzt genau eine Lösung, falls $v_0$ stetig ist. 
\end{thm}
\begin{proof}
Wir wollen \ref{thm:existenz_eindeutigkeit_loesung} anwenden. Zunächst müssen wir alle Voraussetzungen prüfen. 
\begin{enumerate}
	\item  $W=H^1(\Omega)$ ist ein Hilbertraum, also auch ein reflexiver Banachraum. 
	\item Nun muss gezeigt werden, dass $C$ nichtleer, abgeschlossen und konvex ist. 
$C$ ist nichtleer, da $0 \in C$. 

Sei $v_n$ eine konvergente Folge in C. Dann gilt $0 \le v_n \le v_0 \hspace{1ex} \forall n \in \N$. Es gilt auch $0 \le \lim_{n \rightarrow \infty} u_n \le v_0$. Also ist $C$ abgeschlossen. 

Um Konvexität von C zu zeigen, sei $0<\lambda<1 $ und $v, w \in C$. Dann gilt $0 \le \lambda v + (1- \lambda) w$, da $\lambda>0$. Außerdem gilt $\lambda v + (1- \lambda) w \le \lambda v_0 + (1- \lambda) v_0 = v_0$. Also ist jede Konvexkombination in $C$ enthalten, $C$ ist konvex. 
	\item $J$ ist strikt konvex. Der Beweis dazu kann durch einfaches nachrechnen geführt werden. Für Stetigkeit gilt dasselbe. 
	\item $J$ ist Gâteaux differenzierbar nach \eqref{lem:J_g_diffbar}
	\item Sei $w \in C$ mit $\|v\|_{H^1(\Omega)} \rightarrow \infty$. Dann gilt
\begin{align*}
	\begin{array}{ll}
		J(v) 	& = \int_{\Omega} \left( v^2 + \epsilon_1 \right) | \nabla u|^2 + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- v \right)^2 \right) \diff x \\
		& = \int_{\Omega} v^2  | \nabla u|^2 + \epsilon_1  | \nabla u|^2  + \nu \left( \epsilon_2 |\nabla v|^2 + \frac{1}{\epsilon_2} \left( 1- 2v + v^2 \right) \right) \diff x \\
		& = \int_{\Omega} v^2  | \nabla u|^2 - \nu \left( \frac{2}{\epsilon_2} v + \frac{1}{\epsilon_2}v^2 \right)  \diff x + \int_{\Omega} \epsilon_1  | \nabla u|^2  + \nu \frac{1}{\epsilon_2}\diff x  \\
		& \hspace{2ex}+  \int_{\Omega} \nu \epsilon_2 |\nabla v|^2 \diff x \\
		& \le \int_{\Omega} v^2  \left(| \nabla u|^2 + \nu \frac{1}{\epsilon_2} \right)   \diff x + c  +  \epsilon_2 \| \nabla v\|_{L^2(\Omega)}^2 \\
		& \le c' \|v\|_{L^2(\Omega)}^2 + c + \epsilon_2 \| \nabla v\|_{L^2(\Omega)}^2 \\
		& \le c'' \left( \|v\|_{L^2(\Omega)}^2 + \| \nabla v\|_{L^2(\Omega)}^2 \right) + c \\
		& \le c'' \|v\|_{H^1(\Omega)}^2   + c \\
		& \rightarrow \infty
	\end{array}
\end{align*}
mit $c,c',c''>0$ passende Konstanten. 
\end{enumerate}
Alle Voraussetzungen aus \ref{thm:existenz_eindeutigkeit_loesung} sind erfüllt, also existiert genau eine Lösung des Optimierungsproblems. 
\end{proof}
  
Nachdem wir nun wissen, dass die Lösung existiert und eindeutig ist, wollen wir das Minimum finden. Dazu brauchen wir Optimalitätsbedingungen. Diese stellt das folgende Theorem auf 

\begin{thm}[Optimalitätsbedingungen]
	Sei $a:=\inf \{J(w)|H(w) \le_P 0\}$. Dann gilt:
	\begin{align*}
		a= \inf\limits_{v \in H^1(\Omega)} J(v)+ \langle H(v), \begin{pmatrix} \lambda \\ \mu \end{pmatrix} \rangle_{H^1(\Omega), H^{-1}(\Omega)}
	\end{align*} 
	mit 
	\begin{align*}
		H:H^1(\Omega) \rightarrow H^1(\Omega) \\
		v \mapsto	\begin{pmatrix} -v \\ v-v_0 \end{pmatrix}
	\end{align*}
\end{thm}
\begin{proof}
	Die Bedingungen aus \ref{thm:kkt_system} müssen gelten.
	Sei 
	\begin{align*}
			P:=\{(v,w) \in H^1(\Omega) \times H^1(\Omega) | v\ge 0 \text{ und } w \ge 0\} \subset H^1(\Omega) \times H^1(\Omega)
	\end{align*}
	$\mathring{P} \neq \emptyset$, da $H^1(\Omega)$ nur stetige Funktionen enthält. Also ist $P$ ein positiver Kegel. 
	
	$J:H^1(\Omega) \rightarrow \R $ sei wie oben definiert. 
	$H$ ist linear, also konvex. 
	
	Das Bild von $J$ enthält ein $\hat{v}$, sodass $H(\hat{v})<_P 0 $ gilt, da es ein $v \in H^1(\Omega)$ geben muss, das echt zwischen $0$ und $v_0$ liegt. 
	
	Außerdem ist $a:=\inf \{J(w)|H(w) \le_P 0\}< \infty$, da J stetig und beschränkt ist. 
	
	Also kann \ref{thm:kkt_system} angewendet werden. Damit existiert $(\mu, \lambda) \in H^{-1}(\Omega) \times H^{-1}(\Omega)$ mit $(\mu,\lambda) \ge 0$ Komponentenweise, sodass 
	\begin{align*}
		a= \inf\limits_{v \in H^1(\Omega)} J(v)+ \langle H(v), \begin{pmatrix} \lambda \\ \mu \end{pmatrix} \rangle_{H^1(\Omega), H^{-1}(\Omega)}
	\end{align*}
\end{proof}

Damit müssen wir nur noch das Minimum der Lagrangefunktion suchen. Dies funktioniert, indem wir die Ableitung bestimmen und 0 setzen. Wir leiten die Lagragefunktion ab und erhalten  $\nabla J(v)+ \lambda - \mu=0$. Ausformuliert sieht das so aus  
\begin{align*}
	& 2  v | \nabla u|^2  - \epsilon_2 2 \Delta v   - \frac{2}{\epsilon_2} (1-v) + \lambda - \mu = 0 & \text{ auf } \Omega \\
	& 2 \epsilon_2 \nabla v \nu  =0 & \text{ auf } \partial \Omega
\end{align*} 

Nun ist alles gegeben, damit das KKT System aufgestellt werden kann. 
\begin{align*}
	\begin{array}{lll}
	 	\multicolumn{3}{l}{  2  \overline{v} | \nabla u|^2  - \epsilon_2 2 \Delta \overline{v}   - \frac{2}{\epsilon_2} (1-\overline{v}) + \lambda - \mu = 0 \text{ auf } \Omega } \\
	 	\multicolumn{3}{l}{2 \epsilon_2 \nabla \overline{v} \nu  =0 \text{ auf } \partial \Omega} \\
	 	\overline{v} \ge a & \mu \ge 0 & \mu \overline{v}=0 \\
	 	\overline{v} \le b & \lambda \ge 0 & \lambda (v_0-\overline{v})=0 \\
	\end{array}
\end{align*}

Die Projektion für die Nebenbedingung lautet nach \eqref{eq:min_max_theorie}:
\begin{align*}
\mu - \lambda = \max\{0, \mu- \lambda + c(\overline{v}-v_0)\}+ \min\{0, \mu- \lambda + c\overline{v}\} \vspace{2ex} \forall c>0
\end{align*}

Daraus ergibt sich eine starke und schwache Formulierung. Die Starke lautet 
 Suche $v \in H^1$, sodass 
\begin{align*}
	\begin{array}{lll}
		\multicolumn{3}{l}{  2  \overline{v} | \nabla u|^2  - \epsilon_2 2 \Delta \overline{v}   - \frac{2}{\epsilon_2} (1-\overline{v}) + \eta = 0 \text{ auf } \Omega } \\
		\multicolumn{3}{l}{2 \epsilon_2 \nabla \overline{v} \nu  =0 \text{ auf } \partial \Omega} \\
		\eta = \max\{0, \eta + c(\overline{v}-v_0)\}+ \min\{0, \eta + c\overline{v}\} \hspace{1ex} \forall c>0
	\end{array}
\end{align*}
Die schwache Formulierung ist dann
\begin{align*}
 	& \int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1 \diff x = 0  \\
	& \int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(\overline{v}-v_0)\}- \min\{0, \eta + c\overline{v}\} \right) \psi_1 \diff x = 0 
\end{align*}
$ \forall c>0, \forall  \psi_1 \in \h $ mit $\eta = \mu - \lambda$

\subsection{Anwendung auf semidifferenzierbare Newton Methode}

Unser Ziel ist es, eine Methode zu finden, wie wir das KKT System lösen können. Betrachten wir also 
\begin{align*}
	%\begin{array}{rcl}
		& G: H^1(\Omega) \times H^1(\Omega) \rightarrow  H^{-1}(\Omega)^2 \\
		& (v, \eta)								\mapsto   
		\begin{pmatrix}
			\int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1 \right) \diff x  \\
			\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
		\end{pmatrix}
	%\end{array}
\end{align*}
Wir wollen $(v, \eta)$ finden, sodass $G=0$. Direkt kann diese Formel nicht gelöst werden, da wir um $G_2$ zu berechnen $(v,\eta)$ benötigen. Dies ist nicht gegeben. Also lösen wir das Problem mit einer Newton Methode. Für jeden Iterationsschritt ist $(v,\eta)$ durch den vorherigen gegeben. Dafür brauchen wir aber die Ableitung von G.  Da $G_2$ offensichtlich keine G\^ateaux-Ableitung hat, brauchen wir das Semidifferenzial. Also wird die semidifferenzierbare Newton Methode gebraucht. 

Sehen wir uns zunächst die $\partial G_1(v,\eta)(h)$ an. Es gilt:

\begin{thm}
	\label{thm:G1_semidiffbar}
	$G_1(v,\eta)$ ist semidifferenzierbar mit 
	\begin{align*}
		& \partial G_{1 v} (v, \eta)(\phi_1) = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& \partial G_{1 \eta} (v, \eta) (\phi_2) = \int\limits_{\Omega} \nu  \phi_2 \psi_1  \diff x
	\end{align*}	
	falls $u$ fest gewählt ist, oder $u \in L^{\infty}$ 
\end{thm}
\begin{proof}
	Nach \ref{lem:semidifferenzierbar_f_differenzierbar} ist $G_1$ $\partial G_1$ semidifferenzierbar, falls $G_1$ stetig Fréchet differenzierbar ist. Bestimmen wir zunächst die Richtungsableitung in Richtung $\phi_1$ bzw $\phi_2$. Diese ist gegeben durch 
	\begin{align*}
		& \partial G_{1 v} (v, \eta)(\psi_1,\phi_1) = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& G_{1 \eta} (v, \eta) (\phi_2) = \int\limits_{\Omega}  \nu \phi_2 \psi_1  \diff x
	\end{align*}
	Die Berechnung wird hier nicht weiter ausgeführt. Die Richtungsableitungen müssen linear und beschränkt sein. 
	Linearität ist einfach zu sehen.
	Für Beschränktheit von $\partial G_{1v}(v,\eta)( \psi_1,\phi_1 ) $ gilt: 
	\begin{align*}
		\|\partial G_{1v}(v,\eta)(\psi_1,\phi_1)\|_{H^{-1}} \le & = \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 +\nu \left( \epsilon_2 2 \nabla \phi_1 \nabla \psi_1  + \frac{2}{\epsilon_2} \phi_1 \psi_1 \right) \diff x \\
		& \le 2 \|\nabla u\|^2 |(\psi_1,\phi_1)_{H^1}|+  \nu \max\{2 \epsilon_2, \frac{2}{\epsilon_2}\} |(\psi_1, \phi_1)_{H^1}| \\
		& \le \left( C + \tilde{C} \|\nabla u\|^2 \right)\|\psi_1\|_{H^1} \|\phi_1\|_{H^1}
	\end{align*}
	Da $u \in L^{\infty}$, ist $\|\nabla u\|^2$ beschränkt.  Damit ist $G_{1v}$ beschränkt. 
	Für Fréchet Differenzierbarkeit muss eine Abschätzung überprüft werden:
	\begin{align*}
		& \|G_1(v + h,\eta) - G_1(v,\eta) - \partial G_{1v}(v,\eta)(h)\| \\
		& =  \| 	\int\limits_{\Omega} 2 \psi_1  (v+h)  | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla (v+h) \nabla \psi_1  - \frac{2}{\epsilon_2} (1-(v+h))\psi_1 + \eta \psi_1  \right)\diff x \\
		& \hspace{3ex} - 	\int\limits_{\Omega} 2 \psi_1  v | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla v \nabla \psi_1  - \frac{2}{\epsilon_2} (1-v)\psi_1 + \eta \psi_1  \right) \diff x \\
		&  \hspace{3ex} - \int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1  \right) \diff x \| \\
		& = \| \int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 + \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1 \right) \diff x -\int\limits_{\Omega} 2 \psi_1  h | \nabla u|^2 \\
		& \hspace{3ex}+ \nu \left( \epsilon_2 2 \nabla h \nabla \psi_1  + \frac{2}{\epsilon_2} h \psi_1 \right) \diff x \| \\
		& = 0
	\end{align*} 
	Da $G_{1v}$ beschränkt und linear ist, ist $G_{1v}$ auch stetig. 
\end{proof}

Für das Semidifferenzial von $G_2$  beweisen wir zunächst ein Lemma

\begin{lem}
	Betrachte $f: \h^2 \rightarrow H^{-1}(\Omega)$ mit 
	\begin{align}
		\label{eq:H}
		(\eta, v ) \mapsto  \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\}
	\end{align}
	Dann ist $f$ semidifferenzierbar mit 
	\begin{align*}
		\frac{\partial f}{\partial \eta} = 
		\left\{
		\begin{array}{ll}
			\{0\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{1\} 		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack 0,1 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right .
	\end{align*}	
	und 
	\begin{align*}
		\frac{\partial f}{\partial v}= 
		\left\{
		\begin{array}{ll}
			\{-c\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{0\}		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack -c,0 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right .
	\end{align*}	
\end{lem}
\begin{proof}
	$f$ kann in einer anderen Form dargestellt werden 
	\begin{align*}
		f(v, \eta) = 
		\left\{
		\begin{array}{ll}
			-c(v-v_0) 	& \text{ falls }  -c(v-v_0) \le \eta \\
			\eta 		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			-cv		& \text{ falls }  \eta \le -c v 
		\end{array}
		\right .
	\end{align*}
	Die Äquivalenz von diese Form von $f$ und \eqref{eq:H}, kann einfach nachgerechnet werden. 
	Betrachten wir zunächst die Ableitung nach $\eta$. Es reicht, die Semidifferenzierbarkeit der einzelnen Abschnitte zu betrachten. Falls jeder Abschnitt semidifferenzierbar ist und die Übergänge auch, so ist $f$ semidifferenzierbar. 
	
	Sei dazu $-c(v-v_0) < \eta$ oder $ \eta < -c v $. Mit \ref{lem:semidiffbar_f_diffbar} gilt, dass, falls $f$ stetig Fréchet differenzierbar ist, $f$ $\partial f$ semidifferenzierbar. Um Fréchet Differenzierbarkeit zu zeigen, bestimmen wir zunächst die Richtungsableitung. Diese ist offensichtlich $0$. Dadurch folgt sofort die Fréchet differenzierbarkeit. 
	
	Sei nun $-cv < \eta < -c(v-v_0) $. Durch \ref{lem:semidiffbar_f_diffbar} müssen wir wieder die Fréchet Differenzierbarkeit überprüfen. Offensichtlich ist die Identität Fréchet differenzierbar. Das Differenzial ist hier 1. 
	
	Sei $\eta =-c(v-v_0)$. Sei zunächst $d>0$. Die Abschätzung \eqref{eq:semidiffbar_abschaetzung} muss gelten. Hier ist $\partial f(\eta+d, v) = \{0\}$ und damit 
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex}= \| -c(v-v_0) +c(v-v_0) \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right)  \text{ für } \| d \|_{\h} \rightarrow 0
	\end{align*}	
	Sei nun $d<0$. Da $d$ nahe an 0 ist, gilt $d>-cv_0$ mit $v_0>0$. Es ist $\partial G_2^{\eta}(\eta+d) = \{1\}$ und damit 
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta,v)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex} = \| -c(v-v_0) +d +c(v-v_0) -d \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right) 
	\end{align*}	
	für $\| d \|_{\h} \rightarrow 0$
	Fehlt nur noch $\eta = -cv$. Sei zunächst $d>0$. Da $d$ nahe an 0 ist, gilt auch $d<cv_0$. Es gilt $\partial f(\eta+d,v) = \{1\}$ und damit 	
	\begin{align*}
		& \sup\limits_{M \in \partial G_2^{\eta}(\eta+d) } \| f(\eta+d,v)-f(\eta)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex} = \| -cv +d +cv -d \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right)  \text{ für } \| d \|_{\h} \rightarrow 0
	\end{align*}	
	Sei nun $d<0$. Es gilt: Es gilt $\partial G_2^{\eta}(\eta+d) = \{0\}$ und damit 	
	\begin{align*}
		& \sup\limits_{M \in \partial f(\eta+d,v) } \| f(\eta+d,v)-f(\eta,v)-M d\|_{H^{-1}(\Omega)} \\
		& \hspace{3ex}= \| -cv+cv \|_{H^{-1}(\Omega)} = 0 = o\left( \| d\|_{\h} \right) 
	\end{align*}
	 für $ \| d \|_{\h} \rightarrow 0$. Damit ist $f$ semidifferenzierbar nach $\eta$. Für die Semidifferenzierbarkeit nach $v$ gilt die Gleiche Rechnung.  
\end{proof}

Das eigentliche Ziel war es, das Semidifferenzial von $G_2$ zu finden. Dieses können wir nun tun

\begin{thm}
	$G_2: \h^2 \rightarrow H^{-1}(\Omega)$ mit 
\begin{align*}
			(v, \eta) \mapsto	\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
\end{align*}
ist semidifferenzierbar mit 
	\begin{align*}
		\partial G_{2 \eta}(\eta, v)(\psi_2, \phi_2) = \int\limits_{\Omega} \frac{\partial f}{\partial \eta} \psi_2 \phi_2 \diff x 
	\end{align*}
	\begin{align*}
		\partial G_{2 v}(\eta, v) (\psi_2, \phi_1) = \int\limits_{\Omega}  \frac{\partial f}{\partial v} \psi_2 \phi_1 \diff x 
	\end{align*}
\end{thm}
\begin{proof}
	Es gilt
	\begin{align*}
		 & \int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x \\
		 & = \int\limits_{\Omega} \eta \psi_2 \diff x - \int\limits_{\Omega} \max\{0, \eta + c(v-v_0)\} \psi_2 \diff x - \int\limits_{\Omega} \min\{0, \eta + c v\} \psi_2 \diff x \\
		 & = F_1(v,\eta)(\psi_2) - F_2(v,\eta)(\psi_2) - F_3(v,\eta)(\psi_2)  
	\end{align*}
	Wir können nach \ref{thm:rechenregeln_semidiffbare_fkt} $F_1$, $F_2$, $F_3$ getrennt ableiten. Betrachten wir die Ableitung nach $\eta$. Die Ableitung von $F_1$ ist einfach
	\begin{align*}
		\partial F_{1 \eta}(\psi_2, \phi_2) = \int_{\Omega} \psi_2 \phi_2 \diff x 
	\end{align*}
	$F_2$ und $F_3$ lassen sich mithilfe der Kettenregel aus \ref{thm:rechenregeln_semidiffbare_fkt} ableiten
	\begin{align*}
		\partial F_{2 \eta}(\psi_2, \phi_2) & = \int\limits_{\Omega} \frac{\partial}{\partial \eta + c(v-v_0)} \max\{0, \eta + c(v-v_0)\} \frac{\partial \eta}{\partial \eta} \psi_2 \diff x \\
		& =  \int\limits_{\Omega} \frac{\partial f_2}{\partial \eta} \psi_2 \phi_2 \diff x \\
		\partial F_{3 \eta}(\psi_2, \phi_2) & = \int\limits_{\Omega} \frac{\partial}{\partial \eta + cv} \min\{0, \eta + cv\} \frac{\partial \eta}{\partial \eta} \psi_2 \diff x \\
			& =  \int\limits_{\Omega} \frac{\partial f_3}{\partial \eta} \psi_2 \phi_2 \diff x
	\end{align*}
	mit $f_1$ und $f_2$ der $\min$ bzw. der $\max$ Term. Die Ableitungen kann man davon einfach berechnen. 
	Es ergibt sich: 
	\begin{align*}
		\partial G_{2 \eta}(\eta, v)(\psi_2, \phi_2) & = \partial F_1 (\eta, v)(\psi_2, \phi_2) + \partial F_2 (\eta, v)(\psi_2, \phi_2) + \partial F_3 (\eta, v)(\psi_2, \phi_2) \\
		& =  \int\limits_{\Omega} \psi_2 \phi_2 \diff x + \int\limits_{\Omega} \frac{\partial f_2}{\partial \eta} \psi_2 \phi_2 \diff x \int\limits_{\Omega} \frac{\partial f_3}{\partial \eta} \psi_2 \phi_2 \diff x \\
		& = \int\limits_{\Omega} (id + \frac{\partial f_2}{\partial \eta}  +\frac{\partial f_3}{\partial \eta} ) \phi_2 \psi_2 \diff x \\
		& = \int\limits_{\Omega} \frac{\partial f}{\partial \eta} \psi_2 \phi_2 \diff x 
	\end{align*} 
	Es fügen sich die Ableitungen von $f_2,f_3$ und $\eta$ wieder zu $\frac{\partial f}{\partial \eta} $ zusammen. 
\end{proof}

Damit ergibt sich als Ableitung 
\begin{align*}
	G'(v,\eta)= 
	\begin{pmatrix}
			G_{1 v} & G_{1 \eta} \\
			G_{2 v} & G_{2 \eta}  
	\end{pmatrix}
\end{align*}
Also lautet das Gleichungssystem, das für das Newtonverfahren nach $s$ gelöst werden muss
\begin{align*}
	- \begin{pmatrix}
		G_1 \\
		G_2
	\end{pmatrix}
	= 
	\begin{pmatrix}
			G_{1 v} & G_{1 \eta} \\
			G_{2 v} & G_{2 \eta}  
	\end{pmatrix}
	\begin{pmatrix}
	s^1 \\
	s^2
	\end{pmatrix}
\end{align*}

Stellen wir dazu die Newton Methode auf

\begin{algorithm}[H]
	\caption{semidiffbare Newton Methode}
	\label{algo:semidiffbare_nm_anw}
	\KwData{$v^0, \eta^0 $ möglichst Nah an der Lösung $\overline{v}, \overline{\eta}$}
	\For{$k=0,1,\cdots$} {
		\emph{Erhalte $s_1^k$ beim lösen von $
			 - \begin{pmatrix}
			 G_1 \\
			 G_2
			 \end{pmatrix}
			 = 
			 \begin{pmatrix}
			 G_{1 v} & G_{1 \eta} \\
			 G_{2 v} & G_{2 \eta}  
			 \end{pmatrix}
			 \begin{pmatrix}
			 s^1 \\
			 s^2
			 \end{pmatrix}$}\;
		$v^{k+1}=v^k+s_1^k \hspace{2ex}  \eta^{k+1}=\eta^k+s_2^k$\;
	}
\end{algorithm}

%Nun wollen wir noch herausfinden, ob die Newton Methode konvergiert.
%
%\begin{thm}[Konvergenz der Newton Methode]
%	Der Algorithmus \ref{algo:semidiffbare_nm_anw} konvergiert.	
%\end{thm}
%\begin{proof}
%	Wir wollen Theorem \ref{thm:konvergenz_des_semidiffbaren_newton_verfahrens} anwenden. Dazu muss $G$ stetig sein und $\partial G$ semidifferenzierbar und die Regularitätsannahme muss erfüllt sein. 
%	Die Semidifferenzierbarkeit wurde in \ref{thm:G1_semidiffbar} schon gezeigt. Fehlt noch die Reguaritätsannahme.  
%	Dazu muss gezeigt werden, dass 
%	\begin{align*}
%		\exists C>0, \quad \exists \delta >0 : \|M^{-1}\|_{X \rightarrow Y} \le C \quad \forall M \in \partial G(x) \quad \forall x \in X, \quad \|x-\overline{x}\|_X<\delta
%	\end{align*}
%	mit 
%	\begin{align*}
%	M:= \begin{pmatrix}
%		\partial G_{1 v} & \partial G_{1 \eta} \\
%		\partial G_{2 v} & \partial G_{2 \eta} 
%	\end{pmatrix}
%	\end{align*} 
%\end{proof}

\subsection{Numerische Betrachtung}

Alle Funktionen aus dem Newtonsystem müssen numerisch dargestellt werden. 

Für die Diskretisierung wird dasselbe Gitter und die Selben Elemente genommen wie bei der Optimierung nach u. Auch hier werden wir wieder mit dem Galerkin Ansatz arbeiten d.h. 
\begin{align*}
	v=\sum\limits_{i=1}^{k} v_i^h T_i \hspace{9ex} \eta = \sum\limits_{i=1}^{k} \eta_i^h T_i \hspace{9ex} v_0=\sum\limits_{i=1}^{k} {v_0}_i^h T_i
\end{align*}
wobei die $T_i$ wieder die globalen Formfunktionen sind. 

Da $u$ gegeben ist, ist $u$ ein Vektor mit den Auswertungen an den Ecken der Dreiecke. Die Darstellung ist die gleiche wie in \eqref{eq:ungerades_dreieck_lineare_fkt} und \eqref{eq:ungerades_dreieck_lineare_fkt}. Also gilt 
\begin{align*}
	|\nabla u|^2= (u_{31}-u_{21})^2+(u_{11}-u_{21})^2+ (u_{32}-u_{22})^2+(u_{12}-u_{22})^2=:u^{dis}
\end{align*}  

\subsubsection{Numerische Darstellung von $G_{1}$}

\begin{align*}
G_1(v,\eta) =  \int\limits_{\Omega} 2 \psi_1 v | \nabla u|^2 + \nu \left(  \epsilon_2 2 \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1 + \right) \eta \psi_1 \diff x\end{align*}
wird nun diskretisiert:
\begin{align*}
	& \hspace{2ex} \int\limits_{\Omega} 2\psi_1 v | \nabla u|^2 + \nu \left( 2 \epsilon_2  \nabla v \nabla \psi_1 - \frac{2}{\epsilon_2} (1-v)\psi_1  \right) +  \eta \psi_1  \diff x \\
	& = \int\limits_{\Omega} 2 T_j  \sum\limits_{i=1}^{k} v_i^h T_i u^{dis} + \nu \left( \epsilon_2 2 \sum\limits_{i=1}^{k} v_i^h \nabla T_i \nabla  T_j - \frac{2}{\epsilon_2} (1-\sum\limits_{i=1}^{k} v_i^h T_i) T_j \right)\\
	&  \hspace{3ex} + \sum\limits_{i=1}^{k} \eta_i^h T_i T_j  \diff x \\
	& =  \sum\limits_{i=1}^{k} v_i^h \left(  2 \int\limits_{\Omega} u^{dis} T_i T_j \diff x + 2 \epsilon_2 \nu \int\limits_{\Omega} \nabla T_i \nabla T_j \diff x +  \nu \frac{2}{\epsilon_2}  \int\limits_{\Omega} T_i T_j \diff x  \right) \\
	& - \nu \frac{2}{\epsilon_2} \sum\limits_{i=1}^{k} \int\limits_{\Omega} T_j \diff x 
	+  \sum\limits_{i=1}^{k} \eta_i^h \int\limits_{\Omega} T_i T_j \diff x \\
	& = (2 A + \nu 2 \epsilon_2 B + \nu \frac{2}{\epsilon_2} D) v^h - \nu \frac{2}{\epsilon_2} D*e + D\eta^h
\end{align*}

mit $e:=(1,\cdots,1)^T v^h:=(v_1^h, \cdots v_k^h)^T$, $\eta^h:=(\eta_1^h, \cdots \eta_k^h)^T$, $A_{ij}=  \int_{\Omega} u^{dis} T_i T_j \diff x $, $B_{ij}:= \int_{\Omega} \nabla T_i \nabla T_j \diff x$, $c_j:= \int_{\Omega} T_j \diff x$ und $D_{ij}:= \int_{\Omega} T_i T_j \diff x $.

Die Berechnung der Matrizen $A,B,D$ und des Vektors $c$ erfolgt im Anhang \ref{sec:rechnungen}. Es ergibt sich 
\begin{equation*}
	(2 A + 2 \nu \epsilon_2 B + \frac{2 \nu }{\epsilon_2} D) v^h  - \frac{2 \nu }{\epsilon_2} De + D \eta^h =  
\end{equation*}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.15]{images/formel.png}	
	\label{fig:formel}
\end{figure}
%todo BildÄndern

wobei bei $A$ auf der Hauptdiagonalen $\frac{1}{12} ( \sum\limits_{i=1}^6 u^{dis}_{E_i} )$ steht, auf der Nebendiagonalen steht $\frac{1}{24} ( u^{dis}_{E_3} + u^{dis}_{E_6} ) $ , auf der zweiten Nebendiagonalen $ \frac{1}{24} ( u^{dis}_{E_4} + u^{dis}_{E_5} ) $  und auf der dritten Nebendiagonalen $ \frac{1}{24} ( u^{dis}_{E_5} +u^{dis}_{E_6} )$. 

Bei $B$ steht  auf der Hauptdiagonalen $4 $, auf der Nebendiagonalen und der zweiten Nebendiagonalen  $-1 $. 

Bei $D$ steht  auf der Hauptdiagonalen $\frac{1}{2} $, auf der ersten, zweiten und dritten Nebendiagonalen $\frac{1}{12} $. 

Durch die noch ausstehende Transformation der Dreiecke, muss der gesamte Term mit $1/ h_1 h_2$ multipliziert werden.  

\subsubsection{Numerische Darstellung von $G_{2}$}
\begin{align*}
	\int\limits_{\Omega} \left( \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\} \right) \psi_2 \diff x
\end{align*}
Um dieses Funktional numerisch darzustellen, benutzen wir den Galerkin Ansatz mit 
\begin{align*}
	& v = \sum\limits_{i=1}^k v_i^h T_i(x,y) \hspace{9ex} \eta = \sum\limits_{i=1}^k \eta_i^h T_i(x,y) \hspace{9ex} v_0 = \sum\limits_{i=1}^k {v_0}_i^h T_i(x,y) 
\end{align*}
Daraus ergibt sich:
\begin{align*}
	& \int\limits_{\Omega} \left( \eta - \max \left\{ 0, \eta + c(v-v_0) \right\} - \min \left\{ 0, \eta + c v \right\} \right) \psi_2 \diff x  \\
	& = \int\limits_{\Omega} \left(  \sum\limits_{i=1}^k \eta_i^h T_i \right. - \max \left\{ 0,  \sum\limits_{i=1}^k \eta_i^h T_i + c(\sum\limits_{i=1}^k v_i^h T_i - \sum\limits_{i=1}^k {v_0}_i^h T_i ) \right\} \\
	& \hspace{5ex} \left. - \min \left\{ 0,  \sum\limits_{i=1}^k \eta_i^h T_i + c \sum\limits_{i=1}^k v_i^h T_i \right\} \right) T_j \diff x  \\ 
	& = \int\limits_{\Omega} \left(  \sum\limits_{i=1}^k \eta_i^h T_i \right. -   \sum\limits_{i=1}^k \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h )  \right\} T_i \\
	& \hspace{5ex} \left. -\sum\limits_{i=1}^k  \min \left\{ 0,  \eta_i^h + c v_i^h    \right\} T_i  \right) T_j \diff x  \\ 
	& = \int\limits_{\Omega}  \sum\limits_{i=1}^k \left(  \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  \right) T_i T_j \diff x \\
	& =  \left( \sum\limits_{i=1}^k   \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  \right) \int\limits_{\Omega} T_i T_j \diff x \\	  
	& = D w_{v \eta}	
\end{align*}
mit $D$ aus der numerischen Darstellung von $G_1$ und\\ $(w_{v \eta})_i:=  \eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}$. Die Summe und $T_i$ darf aus dem $\max$ bzw. $\min$ herausgezogen werden, da $T_i$ immer nur an einem Punkt ungleich 0 ist. Dadurch kommt niemals zustande, dass mehr als ein Term der Summe ungleich 0 ist. 
$w_{v \eta}	$ kann auch explizit dargestellt werden: 
\begin{align*}
	(w_{v \eta})_i = 
		\left\{
			\begin{array}{ll}
				- c(v_i^h - {v_0}_i^h ) 	& \text{ falls }  -c(v_i^h - {v_0}_i^h ) \le \eta_i^h \\
				\eta_i^h 		& \text{ falls } -c v_i^h  < \eta < -c(v_i^h - {v_0}_i^h ) \\
				-c v_i^h	& \text{ falls }  \eta_i^h \le -c v_i^h
			\end{array}
		\right..
\end{align*}
Auch hier muss das Integral wieder transformiert werden, wodurch der Faktor $^/h_1 h_2$ multipliziert wird. 

\subsubsection{Numerische Darstellung von $G_{1 v}$}
\begin{align*}
 	G_{1 v} (v, \eta) = \int\limits_{\Omega} 2  \psi_1  \phi_1 | \nabla u|^2 + \nu 2 \epsilon_2 \nabla \phi_1 \nabla \psi_1  + \frac{2 \nu}{\epsilon_2} \phi_1 \psi_1 \diff x
\end{align*}

wird nun diskretisieren:

\begin{align*}
	& \int\limits_{\Omega} 2 \psi_1  \phi_1 | \nabla u|^2 + 2 \nu \epsilon_2  \nabla \phi_1 \nabla \psi_1  + \frac{2 \nu}{\epsilon_2} \phi_1 \psi_1 \diff x\\
	& = \int\limits_{\Omega} 2  T_j  T_i u^{dis} + 2 \nu \epsilon_2  \nabla  T_i  \nabla  T_j  + \frac{2 \nu}{\epsilon_2}  T_i T_j \diff x \\
	& = 2 A  + 2 \nu \epsilon_2  B + \frac{2 \nu}{\epsilon_2}  D 
\end{align*}

Wir benutzen die gleichen Notationen, wie bei der numerischen Darstellung von $G_1$ und die gleiche Transformation. 

\subsubsection{Numerische Darstellung von $G_{1 \eta}$}
\begin{align*}
 	G_{1 \eta} (v, \eta) = \int\limits_{\Omega}  \phi_2 \psi_1  \diff x
\end{align*}

wird nun diskretisieren:
\begin{align*}
	& \int\limits_{\Omega}  \phi_2 \psi_1 \diff x = \int\limits_{\Omega} T_j  T_i =  D 
\end{align*}

Auch hier muss wieder transformiert werden. 

\subsubsection{Numerische Darstellung von $G_{2 v}$}
Es soll 
	\begin{align*}
		\partial G_{2 v}(\eta, v) (\psi_2, \phi_1) = \int\limits_{\Omega}  \frac{\partial f}{\partial v} \psi_2 \phi_1 \diff x 
	\end{align*}
mit 
	\begin{align*}
		\frac{\partial f}{\partial v}= 
		\left\{
		\begin{array}{ll}
			\{-c\}		& \text{ falls }  -c(v-v_0) < \eta \text{ oder }  \eta < -cv \\
			\{0\}		& \text{ falls } -cv < \eta < -c(v-v_0) \\
			\lbrack -c,0 \rbrack	& \text{ falls }  -c(v-v_0) = \eta \text{ oder }  \eta = -cv 
		\end{array}
		\right..
	\end{align*}
numerisch dargestellt werden. Statt $\frac{\partial f}{\partial v}$ implementieren wir eine Vereinfachung, die nicht mehr Mengenwertig ist. Dazu wählen wir statt $\lbrack -c,0 \rbrack$ einen Punkt aus dem Intervall z.B. $-c/2$. Nun kann  $\frac{\partial f}{\partial v}$  diskretisiert werden zu $f^h$. Dies ist einfach die Funktion ausgewertet an den Gitterpunkten.

Nun wird $\partial G_{2 v}(\eta, v) (\psi_2, \phi_1)$ diskretisiert. Hier wird wie immer $\psi_2, \phi_1$ durch die globalen Formfunktionen $T_i$ ersetzt und $\Omega $ durch die Vereinigung aller Dreiecke. Nun kann für jedes Dreieck  $\int\limits_E  \frac{\partial f}{\partial v} T_i T_j \diff x $ berechnet werden. Dies erfolgt im Anhang \ref{sec:rechnungen}. 
 
Wir erhalten wieder eine Matrix folgender Form:
\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/matrix2.png}
	\caption{Darstellung der Matrix $ \left( \int_{\Omega} (v^2+ \epsilon) \nabla T_i \nabla T_j \right)_{ij}$ }
	\label{fig:int_g2v}
\end{figure}
%todo matrix erstellen. 
Jeder Eintrag, der nicht 0 ist, besteht aus der Summe von unterschiedlichen Auswertungen der Funktion $f$ auf den Dreiecken, über denen integriert wurde. 

\subsubsection{Numerische Darstellung von $G_{2 \eta}$}
Die numerische Darstellung ist genau die gleiche, wie bei $G_{2 v}$, nur dass die Funktionsauswertungen von $f$ andere sind. 

\subsection{Agregation}
Nun sind alle Funktionen diskretisiert und das Problem kann implementiert werden. Stellen wir die genaue Newton Methode auf 

\begin{algorithm}[H]
	\caption{semidifferenzierbare Newton Methode}
	\label{algo:semidiffbare_nm_anw}
	\KwData{$v^0, \eta^h $ (möglichst Nah an der Lösung $(\overline{v},\overline{\eta})$)}
	\For{$k=0,1,\cdots$} {
		\emph{Löse das Gleichungssytem $L u^k = L u_0^k$ nach $u^k$ }\;
		\emph{Erhalte $s_k$ beim lösen von $ -\begin{pmatrix}
			G_1(v^k,\eta^k) \\ G_2(v^k,\eta^k)
			\end{pmatrix} = \begin{pmatrix}
				\partial G_{1v}(v^k, \eta^k)(T_i) & \partial G_{1\eta}(v^k, \eta^k)(T_i) \\
				\partial G_{2v}(v^k, \eta^k)(T_i) & \partial G_{2 \eta}(v^k, \eta^k)(T_i) \\  
			\end{pmatrix} 
			\begin{pmatrix}
			s^k_1 \\ s^k_2_
			\end{pmatrix}$}\;
		$v^{k+1}=v^k+s_1^k \hspace{2ex} \eta^{k+1}= \eta^k + s_2^k$\;
	}
\end{algorithm}
mit
\begin{align*}
	\begin{array}{rcl}
		L 					& = & \int\limits_{\Omega} (v^2 + \epsilon_1) \nabla T_i \nabla T_j \diff x \\
		G_1 				& = &  \left(2 (\int\limits_{\Omega} u^{dis} T_i T_j \diff x)_{ij} + 2 \nu \epsilon_2 (\int\limits_{\Omega} \nabla T_i \nabla T_j \diff x)_{ij} +  \frac{2 \nu}{\epsilon_2}  (\int\limits_{\Omega} T_i T_j \diff x)_{ij}  \right) v^k \\
		& & - \frac{2 \nu}{\epsilon_2} (\int\limits_{\Omega} T_j \diff x)_j +  ( \int\limits_{\Omega} T_i T_j \diff x )_{ij} \eta^k \\
		G_2 				& = &   ( \int\limits_{\Omega} T_i T_j \diff x)_{ij}     (\eta_i^h- \max \left\{ 0,  \eta_i^h + c(v_i^h - {v_0}_i^h ) \right\} -  \min \left\{0,  \eta_i^h + c v_i^h    \right\}  )_i\\
		\partial G_{1v} 	& = & 2 (\int\limits_{\Omega} u^{dis} T_i T_j \diff x)_{ij} + 2 \nu \epsilon_2 (\int\limits_{\Omega} \nabla T_i \nabla T_j \diff x)_{ij} +  \frac{2 \nu}{\epsilon_2}  (\int\limits_{\Omega} T_i T_j \diff x)_{ij}\\
		\partial G_{1\eta}	& = & (\int\limits_{\Omega} T_i T_j \diff x)_{ij}\\
		\partial G_{2v} 	& = & (\int\limits_{\Omega} \frac{\partial f}{\partial v} T_i T_j)_{ij} \\
		\partial G_{2 \eta} & = & (\int\limits_{\Omega} \frac{\partial f}{\partial \eta} T_i T_j)_{ij} \\
		f					& = &  \eta - \max\{0, \eta + c(v-v_0)\}- \min\{0, \eta + c v\}				
	\end{array}
\end{align*}
$1/h_1 h_2$ kommt nicht in der Newton Methode vor, da es vorher gekürzt wurde. 
