\chapter{Mathematische Grundlagen}
\label{chap:mathematische_grundlagen}

Damit eine Lösung des Optimierungsproblems gefunden werden kann, Grundlagen in Optimierung, partieller Differentialgleichungen, Finiter Elemente und semidifferenzierbarer Newton Methoden gebraucht. Diese werden hier eingeführt. 

\section{Optimierung}

Das Phasenfeldmodell für die Rissentstehung ist ein Optimierungsproblem mit Ungleichungsnebenbedingung. Um die Eindeutigkeit und Existenz einer Lösung zu zeigen, werden Grundlagen in der Optimierung benötigt. Außerdem werden Bedingungen vorgestellt, mit denen sich das Optimierungsproblem in eine Nullstellensuche umschreiben lässt. Grundsätzlich lassen sich Optimierungsprobleme in Probleme mit und ohne Nebenbedingung aufteilen. 

\subsection{Optimierungsproblem ohne Nebenbedingung}

Optimierungsprobleme ohne Nebenbedingung kennt man im Endlichdimensionalen bereits aus der Schule. Ein Minimum oder Maximum soll gefunden werden, wozu die zu optimierende Funktion abgeleitet und Null gesetzt wird. Für etwas kompliziertere Probleme reicht Optimierung im Endlichdimensionalen nicht aus. Grundlagen für die unendlichdimensionale Optimierung werden benötigt.

Sei also $W$ ein Banachraum und $J:W \rightarrow \R$ ein Funktional. Das Optimierungsproblem ohne Nebenbedingung hat folgende Form
\begin{align}
	\label{eq:op_ohne_nb}
	\min_{w \in W} J(w)
\end{align}
Damit die Ableitung bestimmt werden kann, muss erst der Ableitungsbegriff in Banachräumen definiert werden. Dies ist die Gâteaux-Ableitung. Die Definitionen stammen aus \citep[S. 50]{hinze:op_pde_constraints}. 

Sei $F: U \subset X \rightarrow Y$ ein Operator zwischen Banachräumen und $U \neq \emptyset$ offen. 

\begin{defi}[Richtungsableitung]
	F heißt Richtungsableitbar in $x \in U$, falls 
	\begin{align*}
		\partial F(x)(h)= \lim\limits_{t \rightarrow 0^+} \frac{F(x+th)-F(x)}{t} \in Y
	\end{align*}
	für alle $h \in X$ existiert. Dann heißt $\delta F(x,h) $ Richtungsableitung von F in Richtung h. 
\end{defi}

\begin{defi}[G\^ateaux differenzierbar]
	F heißt Gâteaux differenzierbar in $x \in U$, falls F richtungsableitbar ist und die Richtungsableitung 
	\begin{align*}
		&  F'(x):X \rightarrow Y \\
		& h \mapsto \partial F(x)(h)
	\end{align*}
	beschränkt und linear ist, d.h. $F'(x) \in L(X,Y)$.
\end{defi}

\begin{defi}[Fr\'echet differenzierbar]
	F heißt Fréchet differenzierbar in $x \in U$, falls F Gâteaux differenzierbar ist und folgende Approximation gilt
	\begin{align*}
		\|F(x+h)-F(x)-F'(x)h\|_Y= o \left( \|h\|_X \right) \text{ für }  \hspace{1ex} \|h\|_X  \rightarrow 0
	\end{align*} 
\end{defi}

Nun kann die Ableitung von $J$ bestimmt werden und daraus resultierend das Optimeriungsproblem gelöst werden. Theorem und Beweis stammen aus der Vorlesung \glqq Optimierung 2\grqq, gelesen von Prof. B. Wirth \cite{op}. 

\begin{thm}
	\label{thm:ableitung_gleich_null}
	Sei das Optimierungsproblem \eqref{eq:op_ohne_nb} gegeben. 
	Sei $J:W \rightarrow \R$ Gâteaux differenzierbar in $\tilde{w} \in W$. Wenn $\tilde{w}$ das Optimierungsproblem löst, gilt 
	\begin{align}
		\label{blub}
		\partial J(\tilde{w})(h)=0 \hspace{2ex} \forall \hspace{1ex}h \in W
	\end{align}
	Dabei ist $h$ die Richtung der Ableitung.  
\end{thm}
\begin{proof}
	Für alle $h \in W$ muss $\alpha \mapsto J(\tilde{w}+ \alpha h)$ minimal in $\alpha = 0$ sein. Daraus folgt
	\begin{align*}
		\frac{\partial}{\partial \alpha} f(x+\alpha h)|_{\alpha  =0} = 0
	\end{align*}
\end{proof}

Oftmals ist \eqref{blub} eine partielle Differentialgleichung. In den Grundlagen partieller Differentialgleichungen \ref{sec:grundlagen_pdgl} wird erklärt, wann eine Lösung existiert und ob diese eindeutig ist. 

\subsection{Optimierungsproblem mit Ungleichungsnebenbedingung}

Oftmals tauchen als Nebenbedingungen Ungleichungsbedingungen wie $a \le u \le b$ auf, wobei $a,b,u \in X$ gilt und $X$ ein Vektorraum ist. Dabei ist $u$ die zu optimierende Funktion, $a$ die untere und $b$ die obere Schranke. Zum besseren Verständnis der Bedeutung des  Ungleichheitszeichens, wird ein positiver Kegel nach der Vorlesung \glqq Optimierung 2\grqq  von Prof. Wirth \cite{op} definiert. 

\begin{defi}[positiver Kegel] 
	Sei X ein Vektorraum, $P \subset X$ ein konvexer Kegel. Für $x,y \in X$ schreiben wir $x \le_P y$ oder $y \ge_p x$, falls $y-x \in P$. P heißt positiver Kegel. 
	
	$x<_P y $ oder $y>_P x$ bedeutet $y-x \in \mathring{P}$ 
\end{defi}

Wir werden Probleme der Form
\begin{align*}
	\min\limits_{w \in W} J(w) \quad s.d. \quad G(w) \le_p 0 
	%\label{eq:optimierungsproblem_funktionbedingung}
\end{align*}

betrachten, wobei $W,Z$  Banachräume sind, $J: W \rightarrow \R $ Gâteaux differenzierbar und  $G: W \rightarrow Z$ die Nebenbedingung des Optimierungsproblems ist. $P \subset Z$ ist ein positiver Kegel. Die Nebenbedingung lässt sich in eine Raumnebenbedingung umschreiben, also $C:=\{w \in W| G(w) \le_P 0 \}$. Dabei ist $C$ nichtleer, abgeschlossen und konvex. Das Problem lautet  
\begin{align}
	\min\limits_{w \in W} J(w) \quad s.d. \quad w \in C 
	\label{eq:optimierungsproblem_raumbedingung}
\end{align}

Je nachdem welche Notation praktischer ist, wird die eine oder andere benutzt. 
Bei Optimierungen dieser Art muss zunächst die Existenz und Eindeutigkeit der Lösung gesichert werden. 

\begin{thm}
	\label{thm:existenz_eindeutigkeit_loesung}
	Sei
	\begin{enumerate}
		\item W reflexiver Banachraum
		\item $C \subset W$ nichtleer, konvex und abgeschlossen
		\item $J: W \rightarrow \R$ strikt konvex und stetig auf C 
		\item $J$ Gâteaux differenzierbar
		\item $\lim\limits_{w \in C, \|w\|_W \rightarrow \infty} J(w)=\infty $
	\end{enumerate}
	Dann existiert genau eine Lösung von \eqref{eq:optimierungsproblem_raumbedingung}.  
\end{thm} 
\begin{proof}
	Der Beweis und das Theorem sind in \cite[S.66]{hinze:op_pde_constraints} zu finden 
\end{proof}

Bei Optimierungsproblemen mit Nebenbedingung reicht als Bedingung für das Optimum nicht aus, dass die Ableitung 0 ist. Da das Optimum auf dem Rand des zulässigen Gebietes sein könnte, muss die Ableitung nicht zwingend 0 sein. Jedoch gibt es andere Bedingungen, die ausreichend für ein Optimum sind. Die Herleitung dieser Bedingungen, die im folgenden Karush-Kuhn-Tucker Bedingungen (kurz: KKT) genannt werden, werden aufgrund des Umfanges nicht eingeführt, sondern nur angegeben.    

\begin{thm}[Lagrangefunktion]
	\label{thm:kkt_system}
	Seien X,Y normierte Räume, $P \subset Z$ ein positiver Kegel mit \r{P}$ \neq \emptyset$. Sei $J:W \rightarrow \R \cup \{\infty\}$, $G:W \rightarrow Z$ konvex. Es existiert ein $\hat{w} $ im Bild(J),  sodass $G(\hat{w})<_P 0$. Außerdem gelte $\mu=\inf\{J(w)|G(w)\le_P 0\}< \infty $. 
	
	Dann existiert $ z' \in Z^*$ mit $ z' \ge_{P^*} 0$, sodass $\mu=\inf_{w \in W} J(w)+ \langle G(w),z'\rangle_{Z,Z^*}$.
	Falls ein optimales $\overline{w}$ existiert, dann minimiert $\overline{w}$  
	\begin{align*}
		J(w)+ \langle G(w),z'\rangle_{Z,Z^*}
	\end{align*}
	 
\end{thm}
\begin{proof}
	Der Beweis ist im Script zur Vorlesung \glqq Optimierung II \grqq, gelesen von Prof. B. Wirth \cite{op}, zu finden. 
\end{proof}

Mit dieser Bedingungen kann von \eqref{eq:optimierungsproblem_raumbedingung} das KKT System aufgestellt werden. Dabei ist $\overline{w}$ die Lösung des Problems. $\mu$ und $\lambda$ sind die Lagrange Multiplikatoren. 
\begin{align}
	\label{bla}
	\begin{array}{lll}
		\multicolumn{3}{l}{ \nabla J(\overline{w})+ \lambda - \mu =0} \\
		\overline{w} \ge a & \mu \ge 0 & \mu (\overline{w}-a)=0 \\
		\overline{w} \le b & \lambda \ge 0 & \lambda (b-\overline{w})=0 \\
	\end{array}
\end{align}
Die unteren beiden Zeilen in \eqref{bla} kann man als Projektion darstellen. Es gilt für alle $c>0$
\begin{align*}
	\mu = \max\{0, \mu + c(\overline{w}-a)\} \\
	\lambda = \max\{0, \lambda + c(b-\overline{w})\} 
\end{align*} 
Daraus ergibt sich nach \cite{kunisch}:
\begin{align}
	\label{eq:min_max_theorie}
	\mu - \lambda & =  \max\{0, \mu + c(\overline{w}-a)\}  -  \max\{0, \lambda + c(b-\overline{w})\} \\ \notag
	& =  \max\{0, \mu + c(\overline{w}-a)\}   + \min\{ - \lambda + c(\overline{w}-b)\} \\ \notag
	& = \max\{0, \mu - \lambda + c(\overline{w}-a)\}   + \min\{ \mu - \lambda + c(\overline{w}-b)\} 
\end{align}
Diese Darstellung wird später benutzt, um das Problem über die Rissentstehung zu lösen. 

\section{Partielle Differentialgleichungen}
\label{sec:grundlagen_pdgl}

Optimierungsprobleme kann man oft umschreiben, sodass, statt dem Optimierungsproblem, eine partielle Differentialgleichung gelöst wird. Dadurch können Rückschlüsse auf die Existenz und Eindeutigkeit vom Optimierungsproblem gezogen werden. Die Theorie, die dazu verwendet wird, ist aus der Vorlesung \glqq partielle Differentialgleichungen\grqq gelesen vom Prof. B. Wirth \cite{pdgl}. 

Betrachte das elliptische Dirichlet-Problem auf einem beschränkten Gebiet \mbox{$\Omega \subset \R^n$}
\begin{align}
\label{eq:pDGL}
	Lu=f \text{ auf } \Omega\\
	u=g \text{ auf } \partial \Omega \notag 
\end{align}
mit $g \in H^1(\Omega)$, $f: \Omega \rightarrow \R $ und $Lu(x):= - \div \left(A(x) \nabla u(x) \right) + b(x) \nabla u(x) + c(x) u(x) $, wobei $A: \Omega \rightarrow \R^{n \times n}$, $b: \Omega \rightarrow \R^n$ und $c: \Omega \rightarrow \R$. 

\begin{defi}[schwache Lösung]
$u \in g + \ho$ heißt schwache Lösung zu \eqref{eq:pDGL}, falls 
\begin{align*}
	B(u,v):= \int\limits_{\Omega} \nabla v^T A \nabla u + b \nabla u  v + c u v \diff x = \int_{\Omega }f v \diff x \hspace{2ex} \forall v \in \ho 
\end{align*}
\end{defi}
Damit eine schwache Lösung eindeutig ist, werden einige Voraussetzungen benötigt. 
\begin{ann}
\label{ann:ex_und_eind}
Es existieren $\lambda, \Lambda, \nu >0$, sodass $ \forall x \in \Omega$, $ \forall \xi, \zeta \in \R^n$ gilt 
\begin{enumerate}
	\item $\xi^T A(x) \xi \ge \lambda |\xi|^2 $
	\item $|\xi^T A(x) \zeta| \le \Lambda |\xi| |\zeta| $
	\item $\lambda^{-2} |b(x)|^2 + \lambda^{-1} |c(x)| \le \nu^2 $
	\item $ c(x) \ge 0 $ 
\end{enumerate}
\end{ann}

\begin{thm}[Eindeutigkeit der schwachen Lösung]
\label{thm:eindeutigkeit_schwach_loesung}
Seien die Annahmen \ref{ann:ex_und_eind} für das Problem \eqref{eq:pDGL} erfüllt. Falls eine schwache Lösung  für \eqref{eq:pDGL} existiert, ist sie eindeutig.  
\end{thm}
\begin{proof}
	Der Beweis wird im Script von Prof. B. Wirth zur Vorlesung \glqq Partielle Differentialgleichungen\grqq \cite{pdgl} geführt. 
\end{proof}
%thm 67 pdgl Script wirth S28

\begin{thm}[Existenz der schwachen Lösung]
\label{thm:existenz_schwache_loesung}
	Sei $\Omega$ beschränkt mit Lipschitz Rand. $A,b,c$ seien beschränkt, $f \in L^2(\Omega)$. Dann existiert eine schwache Lösung \mbox{$u \in H^1(\Omega)$} von \eqref{eq:pDGL}. 
\end{thm}
\begin{proof}
	Der Beweis wird im Script von Prof. B. Wirth zur Vorlesung \glqq Partielle Differentialgleichungen\grqq \cite{pdgl} geführt.  
\end{proof}
%Theorem 71 pDGL Script Wirth S.29


\section{Finite Elemente}
\label{sec:finite_elemente}

Finite Elemente sind die Grundlage, um partielle Differentialgleichungen auf zweidimensionalen Gebieten numerisch darstellen zu können. Dazu wird zunächst das Gebiet in Dreiecke trianguliert. Dann werden Basisfunktionen auf diesen Dreiecken definiert, die sogenannten globalen Formfunktionen. Aus diesen ist die gesuchte Funktion zusammengesetzt und kann damit berechnet werden. Die hier beschriebene Theorie richtet sich nach der Vorlesung \glqq Numerik partieller Differentialgleichungen\grqq gelesen von Dr. F. Wübbeling \cite{npdgl}. 

Es ist ein rechteckiges Gebiet in 2D gegeben. O.b.d.A. $\Omega = [0,a] \times [0,b]$. Auf dieses Gebiet legen wir ein äquidistantes Gitter $G_h$. 
\begin{align*}
	G_h:= \left\{ (ih_1,jh_2)| i=0, \cdots , \frac{a}{h_1}, j=0, \cdots , \frac{b}{h_2}  \right\}
\end{align*}
$h=(h_1,h_2)$ ist die Schrittweite mit $a=(n+1) h_1$ und $b=(m+1) h_2$, $n+1$ die Anzahl der Stützpunkte in x-Richtung und $m+1$ die Anzahl der Stützpunkte in y-Richtung.
Um ein sinnvolles Gitter zu erhalten, sollten $h_1$ und $h_2$ recht nahe beieinander gewählt werden. 
Nun wird durch die Gitterpunkte die Triangulierung gelegt. Diese nennen wir $E_k$ und ist in Abbildung  \ref{fig:triangulierung} dargestellt. 
%todo m+1 -> m überall? einfachere notation!

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.1]{images/triang.png}
	\caption{Triangulierung eines rechteckigen Gebietes}
	\label{fig:triangulierung}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/referenzdreieck.png}
	\caption{Referenzdreieck}
	\label{fig:referenzdreieck}
\end{figure}

Um das Referenzelement aufstellen zu können, werden dreieckig lineare Lagrange Elemente genutzt. Bei diesen sind die Funktionsauswertungen auf den Ecken der Dreiecke gegeben. Das Finite Element ist gegeben durch $(E,P, \Psi)$, wobei $E$ das Referenzdreieck \ref{fig:referenzdreieck} ist, $P= \mathcal{P}_1$, Polynome auf $\R^2$ vom Grad 1 mit Basis $ \{p_1, p_2, p_3 \}$,
\begin{align*}
	p_1(x,y):=1, \hspace{5ex} p_2(x,y):=x, \hspace{5ex} p_3(x,y):=y 
\end{align*}
und $\Psi:=\{\varphi_0, \varphi_1, \varphi_2\}$ Funktionale auf $P$ und damit eine Basis von $P^*$ sind. $\varphi_i$ nennt man lokale Formfunktionen und es muss gelten: $\varphi_i(p_j)= \delta_{ij}$, $i,j \in \{ 1,2,3 \}$. Dabei ist $\delta_{ij}$ das Kronecker-Delta. Außerdem soll gelten $\varphi_i(p_j)=p_j(a_i)$, wobei $a_i$ eine Auswertung in einer Ecke des Dreiecks ist. Daraus ergibt sich, dass 
\begin{align}
	\label{eq:varphi}
	\varphi_1=1-x-y, \hspace{6ex} \varphi_2=x, \hspace{6ex} \varphi_3=y
\end{align} 
Jedes Finite Element $(E_k, P_k, \Psi_k)$ lässt sich mit der affin linearen Transformation 
\begin{align*}
	\begin{array}{lrcl}
	T: 	& \R^2 									& \rightarrow 	& \R^2 \\
	& \begin{pmatrix} x \\ y \end{pmatrix}	& \mapsto		& \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} \pm \begin{pmatrix} h_1 x \\ h_2 y \end{pmatrix}
	\end{array}	
\end{align*}
durch das Referenzelement darstellen. Dabei entspricht $(a_1,a_2)^t$ dem Eckpunkt mit dem $90^{\circ}$ Winkel des Dreiecks und $(h_1,h_2)^t$ ist die Höhe bzw. Breite des Dreiecks. Mit dem Transformationssatz können alle Berechnungen auf dem Referenzelement ausgeführt werden und dann auf das transformierte Element übertragen werden. Durch die Transformation muss zu allen Integralen der Term $|\det D T(x,y)|^{-1}$ multipliziert werden. Das ergibt
\begin{align*}
	|\det \text{D } T(x,y)|^{-1} = |\det \begin{pmatrix}
		h_1 & 0 \\ 0 & h_2
	\end{pmatrix}|^{-1} = \frac{1}{h_1 h_2} 
\end{align*} 

Die Familie $\{(E_k,P_k, \Psi_k)\}$ von Finiten Elementen, die durch unsere Triangulierung hervorgegangen ist, ist verträglich. Deshalb können die globalen Formfunktionen aufgestellt werden, die auf dem gesamten Gebiet $\Omega$ definiert sind. Die globale Formfunktion $T_j$ ist $1$ auf dem Gitterpunkt $j$ und 0 sonst. 

Für die Berechnung von linearen Funktionen auf dreieckig linearen Lagrange Elementen brauchen wird teilweise eine explizite Darstellung benötigt. Durch die Triangulierung ergeben sich zwei Arten von Dreiecken. Dabei entspricht $a_i$ dem Wert der Funktion $a: \R^2 \rightarrow \R$ an dem Eckpunkt $i$. 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.25]{images/dreieck_oben_unten_u0.png}	
	\caption{Gerade (links) und ungerade Dreiecke mit den Werten von $a$ }
	\label{fig:oben_unten_dreieck}
\end{figure}

$a(x,y)$ wird auf dem linken Dreieck von Abbildung \ref{fig:oben_unten_dreieck} dargestellt durch  
\begin{align}
	\label{eq:gerades_dreieck_lineare_fkt}
	a(x,y)=(a_3-a_2)x + (a_1 - a_2)y + a_2 \hspace{1ex} \text{ mit } \hspace{1ex} 
		\nabla a(x,y)= 
	\begin{pmatrix}
		a_3 - a_2 \\
		a_1 - a_2 
	\end{pmatrix}
\end{align}

und auf dem rechten Dreieck von \ref{fig:oben_unten_dreieck} wird $a(x,y)$ dargestellt durch
\begin{align}
	\label{eq:ungerades_dreieck_lineare_fkt}
a(x,y)=(a_1-a_2)x + (a_3 - a_2)y + a_2  \hspace{1ex} \text{ mit } \hspace{1ex} 
	\nabla a(x,y)= 
	\begin{pmatrix}
		a_1 - a_2 \\
		a_3 - a_2 
	\end{pmatrix}
\end{align}

\section{Semidifferenzierbare Newton Methode}

Semiglatte Newton Methoden werden gebraucht, um Nullstellen von nicht differenzierbaren Funktionen numerisch zu berechnen. Die Rissentstehung ist ein nicht differenzierbares Problem. Um die Idee der Newton Methoden zu verstehen, werden zunächst die einfache Newton Methode ohne Nebenbedingung und dann solche mit einfachen Nebenbedingungen eingeführt. Um diese realisieren zu können, wird der Begriff der Semidifferenzierbarkeit benötigt. Das ist eine Mengenwertige Ableitung, mit der auch nicht differenzierbare, aber stetige Punkte in einer Funktion abgeleitet werden können. Damit kann die semidifferenzierbare Newton Methode eingeführt werden, von der wir auch die Konvergenz betrachten werden.  Dieses Kapitel richtet sich nach \grqq Optimazation with PDE Constraints\grqq  von M. Hinze u.a. \cite[S. 115 ff]{hinze:op_pde_constraints}. 

\subsection{Newton Methode ohne Nebenbedingung}
Als Erstes leiten wir uns zum Verständnis der semidifferenzierbaren Newton Methode die einfache Newton Methode her. Dazu betrachten wir wie vorher das Minimierungsproblem 
\begin{align}
	\label{eq:min}
	\min\limits_{w \in \R^n} f(w) \hspace{6ex} f: \R^n \rightarrow \R
\end{align}
Die Optimalbedingung zu diesem Problem lautet $\nabla f(w)=0$. Zweitens wollen wir ein numerisches Verfahren für dieses Problem entwickeln. Dazu setzen wir $G:= \nabla f$. Da wir ein diskretes Verfahren wollen, setzten wir $w_0, w_1, \hdots $ in $G$ ein. Wir erhalten 
\begin{align*}
	G(w_{k+1})=0
\end{align*}
Um ein iteratives Verfahren zu erhalten taylorn wir $G$ in $w_k$. Das ergibt 
%todo tylorn vll komisch
\begin{thm}[einfaches Newtonverfahren]  
	Der Algorithmus \ref{algo:einfache_nm} löst das Optimierungsproblem \eqref{eq:min}. Es konvergiert superlinear falls $G \in C^1$ und $G'$ invertierbar ist.
	
	\begin{algorithm}[H]
		\caption{einfache Newton Methode}
		\label{algo:einfache_nm}
		\KwData{$w^0$ (möglichst nah an der Lösung $\overline{w}$)}
		\For{$k=0,1,\hdots$} {
			\emph{Löse $G'(w^k) s^k=-G(w^k)$}\;
			$w^{k+1}=w^k+s^k $\;
		}
	\end{algorithm}
\end{thm}

\subsection{Konvergenz der generalisierten Newton Methode}
Betrachten wir ein unendlichdimensionales Optimierungsproblem 
\begin{align}
	\label{eq:g=0}
	G(x)=0
\end{align}
mit $G:X \rightarrow Y$, wobei $X,Y$ Banachräume sind. Sei $\overline{x}$ die Lösung der Gleichung. 

Um eine numerische Lösung von \eqref{eq:g=0} zu erhalten, benutzen wir einen ähnlichen Algorithmus, wie den für das einfache Newtonverfahren, nur allgemeiner: 

\begin{algorithm}[H]
	\caption{Generalisierte Newton Methode}
	\label{algo:generalisierte_newton_methode}
	\KwData{$x^0 \in X$ (möglichst nah an der Lösung $\overline{x}$)}
	\For{$k=0,1,\hdots$} {
		\emph{Wähle invertierbaren Operator $M_k \in L(X,Y)$}\;
		\emph{Erhalte $s_k$ beim Lösen von $M_ks^k=-G(x^k)$}\;
		$x^{k+1}=x^k+s^k $\;
	}
\end{algorithm}

Bis jetzt war der Operator $M_k$ die Ableitung von $G$. Dies ist jedoch nicht möglich, wenn $G$ nicht differenzierbar ist. Wie der Operator $M_k$ in diesem Fall sinnvoll zu wählen ist, wird später bestimmt. 

Nun untersuchen wir die durch diesen Algorithmus gewonnene Folge $x^k$ in einer Umgebung von $\overline{x}$. Sei $d^{k+1} =x^{k+1}-\overline{x}$ der Abstand zwischen dem Iterationsschritt und der Lösung. Dann gilt 
\begin{align*}
	M_kd^{k+1} & = M_k(x^{k+1}-\overline{x})=M_k (x^k+s^k-\overline{x})=M_kd^k-G(x^k) \\
	& = G(\overline{x}) + M_k d^k-G(x^k)
\end{align*}
Wir erhalten 

\begin{thm}[Konvergenz der generalisierten Newton Methode]
	\label{thm:konvergenz_generalisierte_NM}
	Betrachte \eqref{eq:g=0} mit der Lösung $\overline{x}$. Sei $x^k$ die Folge, die durch den Generalisierten Newton Algorithmus \ref{algo:generalisierte_newton_methode} erzeugt wurde. Sei $x^0$ nah genug an $\overline{x}$ gewählt
	\begin{enumerate}
		\item Falls $\exists \gamma \in (0,1)$ mit
		\begin{align*}
			& \| d^{k+1}\|_X =\| M_k^{-1} \left(G(\overline{x}+d^k)-G(\overline{x})-M_kd^k \right) \|_X \le \gamma \| d^k\|_X  \\
			& \forall k \text{ mit } \| d_k\|_X \text{ klein genug}  
		\end{align*}
		gilt, dann konvergiert $x^k \rightarrow \overline{x}$ linear mit Konstante $\gamma$.
		\item Falls $\forall \eta \in (0,1) \quad \exists \delta_{\eta}>0$, sodass
		\begin{align*}
			& \| d^{k+1}\|_X =\| M_k^{-1} \left(G(\overline{x}+d^k)-G(\overline{x})-M_kd^k \right) \|_X  \le \eta \|d^{k+1}\|_X \\ 
			& \text{ für } \| d_k\|_X < \delta_{\eta}
		\end{align*}
		gilt, dann konvergiert $x^k \rightarrow \overline{x}$ super linear.
	\end{enumerate}
\end{thm}
\begin{proof}
	Der Beweis ist in \cite[S. 118]{hinze:op_pde_constraints} zu finden. 
\end{proof}

Oft teilt man diese Kleinheitsannahmen in zwei Teile auf: 

\begin{defi}[Regularitätsannahme]
	Sei $M_k \in L(X,Y)$, wobei X und Y Banachräume sind. Dann ist die Regularitätsannahme gegeben durch:
	\begin{align*}
		\|M_k^{-1}\|_{Y \rightarrow X} \le C \quad \forall k \ge 0
	\end{align*}
\end{defi}

\begin{rem}[Operatornorm]
	Die Notation für die Operatornorm von einem linearen Operator  $ f:X \rightarrow Y$, wobei $X,Y$ normierte Vektorräume sind lautet:
	\begin{eqnarray*}
		\| f\|_{X \rightarrow Y}:=\sup\limits_{\| x\|_X=1} \| f(x)\|_Y
	\end{eqnarray*}
\end{rem}

\begin{defi}[Approximationsannahme]
	Sei $M_k \in L(X,Y)$, wobei X,Y Banachräume sind, $\overline{x}$ die Lösung von $G(x)=0$ und $d^k:=x^k-\overline{x}$.  Sei $\alpha +1>1 $ Dann ist die Approximationsannahme gegeben durch:
	\begin{align*}
		\|G(\overline{x}+d^k)-G(\overline{x})-M_kd^k\|_{Y} = o(\| d^k\|_X) \text{ für } \| d_k\|_X \rightarrow 0
	\end{align*}
\end{defi}


\subsection{Semidifferential}
Die geeingnete Wahl von $M_k$ ist das sogenannte Semidifferential 

\begin{defi}[verallgemeinerte Differentiale]
	Seien $X$ und $Y$ Banachräume und \mbox{$G: X \rightarrow Y$} ein stetiger Operator. Dann ist die Menge der verallgemeinerten Differentiale definiert als 
	\begin{align*}
		\partial G: X \rightrightarrows L(X,Y)
	\end{align*}
\end{defi}
Dabei meint $X \rightrightarrows L(X,Y)$, dass ein Punkt $x \in X$ auf eine Menge von linearen Operatoren abgebildet wird. 

Nun können wir, um in unser Newtonverfahren das Semidifferential zu verwenden, $M_k \in \partial G(x^k)$ wählen. Damit unser Verfahren aber super linear konvergiert, muss gelten 
\begin{align*}
	\sup\limits_{M \in \partial G(\overline{x}+d) } \| G(\overline{x}+d^k)-G(\overline{x})-M_kd\|_Y = o\left( \| d\|_X \right)  \text{ für } \| d\|_X \rightarrow 0
\end{align*}

Dieses nennt sich semidifferenzierbar. 

\begin{defi}[semidifferenzierbar]
	Sei $G: X \rightarrow Y$ ein stetiger Operator zwischen Banachräumen. Sei $\partial G: X \rightrightarrows L(X,Y) $ mit nicht leeren Bildern gegeben wie oben.
	 G heißt $\partial G $ semidifferenzierbar in $x \in X$, falls
		\begin{align}
			\label{eq:semidifferenzierbar_abschaetzung}
			\sup\limits_{M \in \partial G(x+d) } \| G(x+d^k)-G(x)-M_kd\|_Y = o\left( \| d\|_X \right)  \text{ für } \| d \|_X \rightarrow 0
		\end{align}

\end{defi} 

\begin{lem}
	\label{lem:semidifferenzierbar_f_differenzierbar}
	Sei $G: X \rightarrow Y$ ein Operator zwischen Banachräumen und stetig Fréchet differenzierbar in einer Umgebung von x. Dann ist G $\{ G' \}$-semidifferenzierbar in x. 
	
	$\{G'\}$ beschreibt den Operator $\{G'\}: X \rightrightarrows L(X,Y)$ mit $\{G'\}(x)=\{G'(x)\}$ 
\end{lem}
\begin{proof}
	\begin{align*}
		& \| G(x+d^k)-G(x)-G'(x+d)d\|_Y \\
		& \le \| G(x+d^k)-G(x)-G'(x)d\|_Y + \| G'(x)d-G'(x+d)d\|_Y \\
		& \le o \left( \|d\|_X \right) +   \| G'(x)-G'(x+d)\|_{X \rightarrow Y} \|d\|_X=  o \left( \|d\|_X \right)
	\end{align*}
	Der zweite Teil des Beweises erfolgt analog, siehe \cite[S. 121]{hinze:op_pde_constraints}
\end{proof}

\begin{thm}[Rechenregeln semidifferenzierbare Funktionen]
	\label{thm:rechenregeln_semidifferenzierbare_fkt}
	Seien $X,Y,Z, X_i, Y_i$ Banachräume. 
	\begin{enumerate}
		\item Falls die Operatoren $G_i: X_i \rightarrow Y_i$ $\partial G_i$-semidifferenzierbar in x sind, dann ist $(G_1,G_2)$ $(\partial G_1, \partial G_2)$-semidifferenzierbar in x.  
		\item Falls die Operatoren $G_i: X \rightarrow Y$ $\partial G_i$-semidifferenzierbar in x sind, dann ist $ G_1+G_2$ $(\partial G_1 +\partial G_2)$-semidifferenzierbar in x.  
		\item Seien $G_1: Y \rightarrow Z$ und $G_2: X \rightarrow Y$  $\partial G_i$-semidifferenzierbar in $G_2(x)$ und in x. Sei außerdem $\partial G_1$ beschränkt in einer Umgebung von $x=G_2(x)$ und $G_2$ Lipschitzstetig in einer Umgebung von x. Dann ist $G= G_1\circ G_2$ $\partial G$-semidifferenzierbar mit 
		\begin{align*}
			\partial G(x)= \left\{ M_1M_2|M_1 \in \partial G_1\left( \partial G_2(x)\right), \quad M_2 \in \partial G_2(x) \right\}
		\end{align*}	  	
	\end{enumerate}
\end{thm}
\begin{proof}
	Der Beweis ist in \cite[S. 122]{hinze:op_pde_constraints} zu finden. 
\end{proof}

\subsection{Semidifferenzierbare Newton Methode}

Mit dem Semidifferential können wir nun die semidifferenzierbare Newton Methode definieren. 

\begin{algorithm}[H]
	\caption{semidifferenzierbare Newton Methode}
	\label{algo:semidifferenzierbare_newton_methode}
	\KwData{$x^0 \in X$ (möglichst nah an der Lösung $\overline{x}$)}
	\For{$k=0,1,\cdots$} {
		\emph{Wähle $M_k \in \partial G(x^k)$}\;
		\emph{Erhalte $s_k$ beim Lösen von $M_ks^k=-G(x^k)$}\;
		$x^{k+1}=x^k+s^k $\;
	}
\end{algorithm}
Damit diese konvergiert, muss die Approximations- und Regularitätsannahme erfüllt sein. 
Die Approximationsannahme ist durch die Semidifferenzierbarkeit gegeben. Es fehlt noch die Regularitätsannahme. 

\begin{defi}[Regularitätsannahme für das semidifferenzierbare Newton Verfahren]
	\label{def:regularitaetsbedingung}
	Betrachte \eqref{eq:g=0} mit der Lösung $\overline{x}$. Dann lautet die Regularitätsannahme
	\begin{align}
		\label{eq:regularitaetsbedingung}
		\exists C>0, \quad \exists \delta >0 : \|M^{-1}\|_{X \rightarrow Y} \le C \quad \forall M \in \partial G(x) \quad \forall x \in X, \quad \|x-\overline{x}\|_X<\delta
	\end{align}
\end{defi}

\begin{thm}[Konvergenz das semidifferenzierbare Newton Verfahrens]
	\label{thm:konvergenz_des_semidifferenzierbaren_newton_verfahrens}
	Sei das Problem \eqref{eq:g=0} gegeben mit der Lösung $\overline{x}$. Seien $X,Y$ Banachräume, $G: X \rightarrow Y$ stetig und $\partial G$ semidifferenzierbar und die Regularitätsannahme \eqref{eq:regularitaetsbedingung} sei erfüllt. Dann existiert $\delta >0$, sodass für alle $x^0 \in X$ mit $\|x^0- \overline{x}\|_X < \delta $   die semidifferenzierbar Newton Methode super linear gegen $\overline{x}$ konvergiert.
\end{thm}
\begin{proof}
	Nach \ref{thm:konvergenz_generalisierte_NM} können wir aus einem Newtonverfahren der Form \ref{algo:generalisierte_newton_methode}, d.h. es gilt $M_k \in \mathcal{L}(X,Y)$, $M_k$ ist invertierbar und 
	\begin{align*}
		\| M_k^{-1} \left(G(\overline{x}+d^k)-G(\overline{x})-M_kd^k \right) \|_X = o( \| d^k\|_X  )
	\end{align*}  
	gilt,folgern, dass das Newtonverfahren super linear konvergiert. Da $M_k \in \partial G$, ist $M_k \in \mathcal{L}(X,Y)$. $M_k$ ist invertierbar, da die Regularitätsannahme gilt. Außerdem gilt mit der Regularitätsannahme und der Semidifferenzierbarkeit
	\begin{align*}
		& \| M_k^{-1} \left(G(\overline{x}+d^k)-G(\overline{x})-M_kd^k \right) \|_X \\
		& \le \| M_k^{-1} \|_X \| \left(G(\overline{x}+d^k)-G(\overline{x})-M_kd^k \right) \|_X \\
		& \le C o( \| d^k\|_X  ) = o( \| d^k\|_X  )
	\end{align*}  
	Also ist \ref{thm:konvergenz_generalisierte_NM} anwendbar. 
\end{proof}
%todo vll kürzen, falls später keine konvergenz bewiesen werden kann. 

Damit haben wir Bedingungen für die Konvergenz der semidifferenzierbaren Newton Methode gefunden. Diese können wir für den Beweis der Konvergenz bei unserer Newton Methode anwenden. 